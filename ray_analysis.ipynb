{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def collect_data(top_folder, ctx_size):\n",
    "    \"\"\" Collects the minimum test loss and corresponding parameters across all subfolders in the main folder. \"\"\"\n",
    "    results = []\n",
    "    # Traverse through each subfolder in the main folder\n",
    "    for submain_folder in os.listdir(top_folder):\n",
    "        main_folder = os.path.join(top_folder, submain_folder)\n",
    "        for subfolder in os.listdir(main_folder):\n",
    "            subfolder_path = os.path.join(main_folder, subfolder)\n",
    "            progress_file = os.path.join(subfolder_path, 'progress.csv')\n",
    "            params_file = os.path.join(subfolder_path, 'params.json')\n",
    "            \n",
    "            # Check if both necessary files exist\n",
    "            if os.path.exists(progress_file) and os.path.exists(params_file):\n",
    "                try:\n",
    "                    # Read progress.csv and find the minimum test loss\n",
    "                    data = pd.read_csv(progress_file)\n",
    "                    data.fillna(0, inplace=True)\n",
    "                    # Read params.json\n",
    "                    with open(params_file, 'r') as file:\n",
    "                        params = json.load(file)\n",
    "\n",
    "                        def check_ctx_size(ctx_size):\n",
    "                            if ctx_size == None:\n",
    "                                return True\n",
    "                            if 'context' in params and params['context'] == ctx_size:\n",
    "                                return True\n",
    "                            if 'for_all_networks' in params and params['for_all_networks'] == ctx_size:\n",
    "                                return True\n",
    "                            return False\n",
    "                        if check_ctx_size(ctx_size) == False:\n",
    "                            continue\n",
    "                        # Collect required params and the corresponding test loss\n",
    "                        result = {}\n",
    "                        param_dict = {\n",
    "                            'n_stores': 'n_stores',\n",
    "                            'context': 'context',\n",
    "                            'warehouse_holding_cost': 'warehouse_holding_cost',\n",
    "                            'warehouse_lead_time': 'warehouse_lead_time',\n",
    "                            'stores_correlation': 'stores_correlation',\n",
    "                            'learning_rate': 'learning_rate',\n",
    "                            'master_neurons': 'master_neurons',\n",
    "                            'store_embedding': 'store_embedding',\n",
    "                            'for_all_networks': 'for_all_networks',\n",
    "                        }\n",
    "                        for key, value in param_dict.items():\n",
    "                            if value in params:\n",
    "                                result[key] = params.get(value)\n",
    "                            else:\n",
    "                                if key == 'warehouse_lead_time':\n",
    "                                    result[key] = 6\n",
    "                                elif key == 'stores_correlation':\n",
    "                                    result[key] = 0.5\n",
    "\n",
    "                        result['best_dev_loss'] = data['dev_loss'].min()\n",
    "                        result['test_loss(at best_dev)'] = data[data['dev_loss'] == result['best_dev_loss']]['test_loss'].iloc[0]\n",
    "                        result['train_loss(at best_dev)'] = data[data['dev_loss'] == result['best_dev_loss']]['train_loss'].iloc[0]\n",
    "                        result['best_test_loss'] = data['test_loss'].min()\n",
    "                        result['best_train_loss'] = data['train_loss'].min()\n",
    "                        result['path'] = subfolder_path\n",
    "                        results.append(result)\n",
    "                    # if 'test_loss' in data.columns:\n",
    "                    #     min_loss = data['test_loss'].min()\n",
    "                    #     # Read params.json\n",
    "                    #     with open(params_file, 'r') as file:\n",
    "                    #         params = json.load(file)\n",
    "                    #         context_size = params.get('context_size')\n",
    "                    #         # Collect required params and the corresponding test loss\n",
    "                    #         result = {\n",
    "                    #             'learning_rate': params.get('learning_rate'),\n",
    "                    #             'best_test_loss': min_loss\n",
    "                    #         }\n",
    "                    #         results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing files in {subfolder_path}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_results_table(main_folder, ctx_size):\n",
    "    \"\"\" Creates a table of the minimum test losses for each combination of learning_rate, context_size, and samples. \"\"\"\n",
    "    data = collect_data(main_folder, ctx_size)\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        return df.sort_values(by='best_dev_loss', ascending=True)\n",
    "    return None\n",
    "\n",
    "def make_the_result_table(paths, context_sizes):\n",
    "    # Placeholder for results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each path and context size, call the function, and store the top row\n",
    "    for num_stores, path in paths.items():\n",
    "        for context_size in context_sizes:\n",
    "            df = create_results_table(path, context_size)\n",
    "            if df is None:\n",
    "                continue\n",
    "            top_row = df.iloc[0]  # Get the top row\n",
    "            # Extract necessary columns and add custom columns\n",
    "            result_row = {\n",
    "                \"# of stores\": num_stores,\n",
    "                \"context size\": context_size,\n",
    "                \"Learning Rate\": top_row['learning_rate'],\n",
    "                \"Train Loss\": top_row['train_loss(at best_dev)'],\n",
    "                \"Dev Loss\": top_row['best_dev_loss'],\n",
    "                \"Test Loss\": top_row['test_loss(at best_dev)'],\n",
    "                # \"path\": top_row['path']\n",
    "            }\n",
    "            results.append(result_row)\n",
    "\n",
    "    # Combine all top rows into a single DataFrame\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic data, one warehouse lost demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m paths \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user/ml4723/Prj/NIC/ray_results/perf/ctx/3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m5\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user/ml4723/Prj/NIC/ray_results/perf/ctx/5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m50\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user/ml4723/Prj/NIC/ray_results/perf/ctx/50\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mmake_the_result_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[4], line 89\u001b[0m, in \u001b[0;36mmake_the_result_table\u001b[0;34m(paths, context_sizes)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_stores, path \u001b[38;5;129;01min\u001b[39;00m paths\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m context_size \u001b[38;5;129;01min\u001b[39;00m context_sizes:\n\u001b[0;32m---> 89\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_results_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 76\u001b[0m, in \u001b[0;36mcreate_results_table\u001b[0;34m(main_folder, ctx_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_results_table\u001b[39m(main_folder, ctx_size):\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Creates a table of the minimum test losses for each combination of learning_rate, context_size, and samples. \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m     78\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mcollect_data\u001b[0;34m(top_folder, ctx_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(progress_file) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(params_file):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# Read progress.csv and find the minimum test loss\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         data\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# Read params.json\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[0;32m-> 1433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/10\",\n",
    "    20: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/20\",\n",
    "    30: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/30\",\n",
    "    50: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/50\"\n",
    "}\n",
    "\n",
    "df = make_the_result_table(paths, [0, 1, 256])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores  context size  Learning Rate  Train Loss  Dev Loss   Test Loss\n",
      "           3             1           0.01    5.673688  5.659562    5.665522\n",
      "           3             2           0.01    5.682550  5.663403    5.671105\n",
      "           3             4           0.01    5.619100  5.612201    5.613837\n",
      "           3             8           0.01    5.621052  5.612698    5.615135\n",
      "           5             1           0.01    5.317029  5.297034    5.303407\n",
      "           5             2           0.01    5.254412  5.235755    5.246651\n",
      "          10             1           0.01   16.467711 16.325007   16.340109\n",
      "          10             2           0.01    9.930729  9.974490  320.575352\n",
      "          10             4           0.01   11.576283 11.530542   12.204801\n",
      "          10             8           0.01    5.789521  5.815527    5.785925\n",
      "          10            16           0.01    5.725805  5.768559    5.731618\n",
      "          20             1           0.01   20.530435 20.416127   19.890959\n",
      "          20             2           0.01   22.865245 22.898006 1461.001031\n",
      "          30             1           0.01   11.464423 11.431392   11.447458\n",
      "          30             2           0.01   19.043363 19.095562   18.209278\n",
      "          30             4           0.01   11.531930 11.513090   11.530273\n",
      "          30             8           0.01   11.959114 11.865753   11.883993\n",
      "          50             1           0.01   17.479147 17.248856   17.457450\n",
      "          50             2           0.01   16.110344 16.058531   16.002594\n",
      "          50             4           0.01    8.153799  8.144768  501.295088\n",
      "          50             8           0.01   16.376186 16.069947   15.872413\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/10\",\n",
    "    20: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/20\",\n",
    "    30: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/30\",\n",
    "    50: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/50\"\n",
    "}\n",
    "\n",
    "df = make_the_result_table(paths, [1, 2, 4, 8, 16, 32, 64, 128])\n",
    "#df = make_the_result_table(paths, [1, 32, 256])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores  context size  Learning Rate  Train Loss   Dev Loss   Test Loss\n",
      "           3             1          0.010    5.630121   5.619005    5.624080\n",
      "           3             2          0.010    5.629419   5.613049    5.615874\n",
      "           3             4          0.010    5.620411   5.610767    5.611920\n",
      "           3             8          0.010    5.619027   5.609878    5.610715\n",
      "           5             1          0.010    5.270876   5.247549    5.257497\n",
      "           5             2          0.010    5.335778   5.302212    5.309347\n",
      "           5             4          0.010    5.258151   5.236567    5.248581\n",
      "           5             8          0.010    5.262214   5.241388    5.251634\n",
      "          10             1          0.010    5.783530   5.812656    5.778496\n",
      "          10             2          0.010    7.789676   7.823373    7.801336\n",
      "          10             4          0.010    5.725203   5.762549    5.726730\n",
      "          10             8          0.010    6.771707   6.801686    6.771661\n",
      "          20             1          0.010   10.255681  10.277586  667.877047\n",
      "          20             2          0.010  153.903867 153.941619 1758.543906\n",
      "          20             4          0.010   14.889991  14.847557   14.924960\n",
      "          20             8          0.010   11.783439  11.685457   11.748588\n",
      "          30             1          0.001    7.426030   7.515393    7.537770\n",
      "          30             2          0.001    5.625520   5.632997    5.625997\n",
      "          30             4          0.001    7.207498   7.208256    7.204516\n",
      "          30             8          0.001    6.834968   6.850723    6.858735\n",
      "          30            16          0.001    5.660037   5.667138    5.659944\n",
      "          30            32          0.001    5.669732   5.676781    5.670547\n",
      "          30            64          0.001    5.685001   5.693402    5.682598\n",
      "          30           128          0.001    5.620178   5.627820    5.620319\n",
      "          50             1          0.001   11.190899  11.161578   11.183985\n",
      "          50             2          0.001   15.853097  15.854741  143.175425\n",
      "          50             4          0.001   12.293397  12.150335   12.618229\n",
      "          50             8          0.010    8.149763   8.140399   18.479069\n",
      "          50            16          0.001    6.631942   6.602454    6.568062\n",
      "          50            32          0.001    7.443890   7.418650    7.443302\n",
      "          50            64          0.010    6.961960   6.924613    7.214376\n",
      "          50           128          0.010    5.451317   5.417160    5.434257\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/10\",\n",
    "    20: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/20\",\n",
    "    30: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/30\",\n",
    "    50: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/50\"\n",
    "}\n",
    "\n",
    "df = make_the_result_table(paths, [1, 2, 4, 8, 16, 32, 64, 128])\n",
    "#df = make_the_result_table(paths, [1, 32, 256])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores  Architecture Class context size  Learning Rate  Train Loss  Dev Loss  Test Loss\n",
      "           3      Symmetry_Aware            1          0.010    5.613812  5.608477   5.609850\n",
      "           3                 GNN            8          0.010    5.615963  5.607577   5.610156\n",
      "           3 GNN Message Passing            4          0.010    5.615473  5.608413   5.608291\n",
      "           3             Vanilla         None            NaN    5.610000  5.610000   5.610000\n",
      "           5      Symmetry_Aware            1          0.010    5.256996  5.235224   5.247245\n",
      "           5                 GNN            8          0.010    5.254068  5.235106   5.245558\n",
      "           5 GNN Message Passing            4          0.010    5.252546  5.233202   5.243820\n",
      "           5             Vanilla         None            NaN    5.250000  5.250000   5.240000\n",
      "          10      Symmetry_Aware            1          0.010    5.712263  5.755829   5.720371\n",
      "          10                 GNN            8          0.010    5.711636  5.750974   5.716066\n",
      "          10 GNN Message Passing            4          0.010    5.710766  5.752113   5.716915\n",
      "          10             Vanilla         None            NaN    5.720000  5.740000   5.720000\n",
      "          20      Symmetry_Aware            1          0.001    5.850531  5.839253   5.818287\n",
      "          20                 GNN            4          0.010    7.032538  6.906576   6.887273\n",
      "          20 GNN Message Passing           64          0.010    5.914821  5.895593   5.875006\n",
      "          20             Vanilla         None            NaN    5.850000  5.870000   5.850000\n",
      "          30      Symmetry_Aware            1          0.001    5.548159  5.556480   5.550539\n",
      "          30                 GNN          128          0.010    5.552123  5.552791   5.548266\n",
      "          30 GNN Message Passing           16          0.001    5.545213  5.552758   5.549949\n",
      "          30             Vanilla         None            NaN    5.580000  5.600000   5.590000\n",
      "          50      Symmetry_Aware          256          0.001    5.389271  5.366309   5.386096\n",
      "          50                 GNN            4          0.010    8.123946  8.117798 344.827975\n",
      "          50 GNN Message Passing          128          0.001    5.356315  5.343441   5.363186\n",
      "          50             Vanilla         None            NaN    5.410000  5.400000   5.420000\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/10\",\n",
    "    20: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/20\",\n",
    "    30: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/30\",\n",
    "    50: \"/user/ml4723/Prj/NIC/ray_results/perf/ctx/50\"\n",
    "}\n",
    "df_ctx = make_the_result_table(paths, [0, 1, 256])\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmetry_Aware\")\n",
    "df_ctx = df_ctx.loc[df_ctx.groupby(['# of stores'])['Dev Loss'].idxmin()]\n",
    "\n",
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/10\",\n",
    "    20: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/20\",\n",
    "    30: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/30\",\n",
    "    50: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN/50\"\n",
    "}\n",
    "df_gnn = make_the_result_table(paths, [1, 2, 4, 8, 16, 32, 64, 128])\n",
    "df_gnn.insert(1, 'Architecture Class', \"GNN\")\n",
    "df_gnn = df_gnn.loc[df_gnn.groupby(['# of stores'])['Dev Loss'].idxmin()]\n",
    "\n",
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/10\",\n",
    "    20: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/20\",\n",
    "    30: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/30\",\n",
    "    50: \"/user/ml4723/Prj/NIC/ray_results/perf/GNN_message_passing/50\"\n",
    "}\n",
    "df_gnn_mp = make_the_result_table(paths, [1, 2, 4, 8, 16, 32, 64, 128])\n",
    "df_gnn_mp.insert(1, 'Architecture Class', \"GNN Message Passing\")\n",
    "df_gnn_mp = df_gnn_mp.loc[df_gnn_mp.groupby(['# of stores'])['Dev Loss'].idxmin()]\n",
    "\n",
    "vanilla = [\n",
    "    {\n",
    "                \"# of stores\": 3,\n",
    "                \"Architecture Class\": \"Vanilla\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": 5.610,\n",
    "                \"Dev Loss\": 5.610,\n",
    "                \"Test Loss\": 5.610,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 5,\n",
    "                \"Architecture Class\": \"Vanilla\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": 5.250,\n",
    "                \"Dev Loss\": 5.250,\n",
    "                \"Test Loss\": 5.240,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 10,\n",
    "                \"Architecture Class\": \"Vanilla\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": 5.720,\n",
    "                \"Dev Loss\": 5.740,\n",
    "                \"Test Loss\": 5.720,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 20,\n",
    "                \"Architecture Class\": \"Vanilla\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": 5.850,\n",
    "                \"Dev Loss\": 5.870,\n",
    "                \"Test Loss\": 5.850,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 30,\n",
    "                \"Architecture Class\": \"Vanilla\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": 5.580,\n",
    "                \"Dev Loss\": 5.60,\n",
    "                \"Test Loss\": 5.59,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 50,\n",
    "                \"Architecture Class\": \"Vanilla\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": 5.410,\n",
    "                \"Dev Loss\": 5.400,\n",
    "                \"Test Loss\": 5.420,\n",
    "            },\n",
    "]\n",
    "df_vanilla = pd.DataFrame(vanilla)\n",
    "\n",
    "df = pd.concat([df_ctx, df_gnn, df_gnn_mp, df_vanilla])\n",
    "architecture_order = ['Symmetry_Aware', 'GNN', 'GNN Message Passing', 'Vanilla']\n",
    "df['Architecture Class'] = pd.Categorical(df['Architecture Class'], categories=architecture_order, ordered=True)\n",
    "df.sort_values(by=['# of stores', 'Architecture Class'], inplace=True)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic data, Transshipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class context size  Learning Rate  Train Loss  Dev Loss  Test Loss\n",
      "           3     Symmetry_Aware            0         0.0100   15.786119 15.745235  38.493707\n",
      "           3     Symmetry_Aware            1         0.0010    6.451584  6.442219   6.436544\n",
      "           3     Symmetry_Aware          256         0.0100    6.215819  6.195688   6.190809\n",
      "           3            Vanilla         None         0.0001    6.202121  6.195605   6.190790\n",
      "           3        Lower bound         None            NaN         NaN       NaN   6.190000\n",
      "           5     Symmetry_Aware            0         0.0010   14.106193 14.152811  34.056048\n",
      "           5     Symmetry_Aware            1         0.0100    6.061970  6.045415   6.036085\n",
      "           5     Symmetry_Aware          256         0.0010    5.751615  5.759176   5.751703\n",
      "           5            Vanilla         None         0.0001    5.755991  5.759228   5.751869\n",
      "           5        Lower bound         None            NaN         NaN       NaN   5.750000\n",
      "          10     Symmetry_Aware            0         0.0010   14.683252 14.683755  38.511749\n",
      "          10     Symmetry_Aware            1         0.0001    9.254070  9.246135   9.235247\n",
      "          10     Symmetry_Aware          256         0.0010    6.055112  6.058756   6.057758\n",
      "          10            Vanilla         None         0.0001    6.067254  6.073804   6.072607\n",
      "          10        Lower bound         None            NaN         NaN       NaN   6.050000\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/transshipment/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/transshipment/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/transshipment/10\",\n",
    "}\n",
    "df_sym = make_the_result_table(paths, [0, 1, 256])\n",
    "df_sym.insert(1, 'Architecture Class', \"Symmetry_Aware\")\n",
    "\n",
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/10\",\n",
    "}\n",
    "df_van = make_the_result_table(paths, [None])\n",
    "df_van.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "\n",
    "lower_bound = [\n",
    "    {\n",
    "                \"# of stores\": 3,\n",
    "                \"Architecture Class\": \"Lower bound\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": None,\n",
    "                \"Dev Loss\": None,\n",
    "                \"Test Loss\": 6.19,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 5,\n",
    "                \"Architecture Class\": \"Lower bound\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": None,\n",
    "                \"Dev Loss\": None,\n",
    "                \"Test Loss\": 5.75,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 10,\n",
    "                \"Architecture Class\": \"Lower bound\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": None,\n",
    "                \"Dev Loss\": None,\n",
    "                \"Test Loss\": 6.05,\n",
    "            },\n",
    "]\n",
    "df_lower_bound = pd.DataFrame(lower_bound)\n",
    "\n",
    "df = pd.concat([df_sym, df_van, df_lower_bound])\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One warehouse lost demand synthetic - different primitive setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_the_result_table_diff_primitive(paths, context_sizes, warehouse_holding_costs, warehouse_lead_times, stores_correlations):\n",
    "    # Placeholder for results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each path and context size, call the function, and store the top row\n",
    "    for num_stores, path in paths.items():\n",
    "        for context_size in context_sizes:\n",
    "            df = create_results_table(path, context_size)\n",
    "            if df is None:\n",
    "                continue\n",
    "            for warehouse_holding_cost in warehouse_holding_costs:\n",
    "                df_hc = df[df['warehouse_holding_cost'] == warehouse_holding_cost]\n",
    "                if df_hc.empty:\n",
    "                    continue\n",
    "                for warehouse_lead_time in warehouse_lead_times:\n",
    "                    df_hl = df_hc[df_hc['warehouse_lead_time'] == warehouse_lead_time]\n",
    "                    if df_hl.empty:\n",
    "                        continue\n",
    "                    for stores_correlation in stores_correlations:\n",
    "                        df_sc = df_hl[df_hl['stores_correlation'] == stores_correlation]\n",
    "                        if df_sc.empty:\n",
    "                            continue\n",
    "                        top_row = df_sc.iloc[0]\n",
    "                        result_row = {\n",
    "                            'warehouse_holding_cost': top_row['warehouse_holding_cost'],\n",
    "                            'warehouse_lead_time': top_row['warehouse_lead_time'],\n",
    "                            'stores_correlation': top_row['stores_correlation'],\n",
    "                            \"context size\": context_size,\n",
    "                            \"Learning Rate\": top_row['learning_rate'],\n",
    "                            \"Train Loss\": top_row['train_loss(at best_dev)'],\n",
    "                            \"Dev Loss\": top_row['best_dev_loss'],\n",
    "                            \"Test Loss\": top_row['test_loss(at best_dev)'],\n",
    "                            \"path\": top_row['path'],\n",
    "                        }\n",
    "                        results.append(result_row)\n",
    "    result_df = pd.DataFrame(results)\n",
    "    min_test_loss = result_df.groupby(['warehouse_holding_cost', 'warehouse_lead_time', 'stores_correlation'])['Test Loss'].transform('min')\n",
    "    result_df['Test Gap %'] = ((result_df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "    result_df.sort_values(by=['warehouse_holding_cost', 'warehouse_lead_time', 'stores_correlation', 'context size'], inplace=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " warehouse_holding_cost  warehouse_lead_time  stores_correlation  context size  Learning Rate  Train Loss  Dev Loss  Test Loss                                                                                                                                                                                      path  Test Gap %\n",
      "                    0.7                    6                 0.5             0          0.010    5.914676  5.907184   5.912594  /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-41-23/run_e7229_00000_0_context=0,learning_rate=0.0100,samples=1,warehouse_holding_cost=0.7000_2024-07-31_02-41-23    1.489974\n",
      "                    0.7                    6                 0.5             1          0.001    5.844910  5.839476   5.841904  /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-41-23/run_e7229_00005_5_context=1,learning_rate=0.0010,samples=1,warehouse_holding_cost=0.7000_2024-07-31_02-41-23    0.276584\n",
      "                    0.7                    6                 0.5            16          0.010    5.836572  5.827852   5.827418 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-41-23/run_e7229_00002_2_context=16,learning_rate=0.0100,samples=1,warehouse_holding_cost=0.7000_2024-07-31_02-41-23    0.027929\n",
      "                    0.7                    6                 0.5            64          0.001    5.824574  5.827352   5.825791 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-41-23/run_e7229_00007_7_context=64,learning_rate=0.0010,samples=1,warehouse_holding_cost=0.7000_2024-07-31_02-41-23    0.000000\n",
      "                    1.0                    6                 0.5             0          0.010    6.003209  5.989507   6.003050  /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-43-31/run_33d8f_00000_0_context=0,learning_rate=0.0100,samples=1,warehouse_holding_cost=1.0000_2024-07-31_02-43-32    2.210574\n",
      "                    1.0                    6                 0.5             1          0.010    5.912353  5.904200   5.905133 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-43-31/run_33d8f_00013_13_context=1,learning_rate=0.0100,samples=2,warehouse_holding_cost=1.0000_2024-07-31_02-43-32    0.543399\n",
      "                    1.0                    6                 0.5            16          0.001    5.874908  5.872784   5.873428 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-43-31/run_33d8f_00006_6_context=16,learning_rate=0.0010,samples=1,warehouse_holding_cost=1.0000_2024-07-31_02-43-32    0.003573\n",
      "                    1.0                    6                 0.5            64          0.001    5.874376  5.871622   5.873218 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-43-31/run_33d8f_00007_7_context=64,learning_rate=0.0010,samples=1,warehouse_holding_cost=1.0000_2024-07-31_02-43-32    0.000000\n",
      "                    1.3                    6                 0.5             0          0.010    6.043122  6.032702   6.045250 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_01-37-33/run_fc58a_00012_12_context=0,learning_rate=0.0100,samples=2,warehouse_holding_cost=1.3000_2024-07-31_01-37-33    2.959561\n",
      "                    1.3                    6                 0.5             1          0.010    5.880304  5.878862   5.880147  /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_01-37-33/run_fc58a_00001_1_context=1,learning_rate=0.0100,samples=1,warehouse_holding_cost=1.3000_2024-07-31_01-37-33    0.147620\n",
      "                    1.3                    6                 0.5            16          0.010    5.871605  5.872480   5.871480 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_01-37-33/run_fc58a_00002_2_context=16,learning_rate=0.0100,samples=1,warehouse_holding_cost=1.3000_2024-07-31_01-37-33    0.000000\n",
      "                    1.3                    6                 0.5            64          0.001    5.879224  5.871810   5.872867 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_01-37-33/run_fc58a_00007_7_context=64,learning_rate=0.0010,samples=1,warehouse_holding_cost=1.3000_2024-07-31_01-37-33    0.023625\n",
      "                    2.0                    6                 0.5             0          0.010    6.216217  6.145926   6.287749  /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-01-22/run_5012f_00000_0_context=0,learning_rate=0.0100,samples=1,warehouse_holding_cost=2.0000_2024-07-31_02-01-22    7.094251\n",
      "                    2.0                    6                 0.5             1          0.010    5.896185  5.874366   5.876068  /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-01-22/run_5012f_00001_1_context=1,learning_rate=0.0100,samples=1,warehouse_holding_cost=2.0000_2024-07-31_02-01-22    0.082399\n",
      "                    2.0                    6                 0.5            16          0.010    5.873271  5.871753   5.871852 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-01-22/run_5012f_00002_2_context=16,learning_rate=0.0100,samples=1,warehouse_holding_cost=2.0000_2024-07-31_02-01-22    0.010600\n",
      "                    2.0                    6                 0.5            64          0.001    5.869408  5.871531   5.871230 /user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/run_2024-07-31_02-01-22/run_5012f_00007_7_context=64,learning_rate=0.0010,samples=1,warehouse_holding_cost=2.0000_2024-07-31_02-01-22    0.000000\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx'\n",
    "}\n",
    "\n",
    "# df = make_the_result_table_diff_primitive(paths, [0, 1, 16, 64], [0.7, 1.0, 1.3, 2.0], [2, 6], [-0.5, 0.0, 0.5])\n",
    "df = make_the_result_table_diff_primitive(paths, [0, 1, 16, 64], [0.7, 1.0, 1.3, 2.0], [6], [0.5])\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('neural_inventory_control')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "295131613949c3a10f810057b447f8c1d2f9ae56e2b2c791b6162e59dbe65d0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
