{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic data, one warehouse lost demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import analysis.ray_results_interpreter as rri\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "\n",
    "from ray.tune import ExperimentAnalysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def merge_experiment_results(paths):\n",
    "    dataframes = []\n",
    "    for path in paths:\n",
    "        analysis = ExperimentAnalysis(path)\n",
    "        dataframes.append(analysis.dataframe())\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns\n",
    "    columns_order = ['time_this_iter_s', 'time_total_s', 'iterations_since_restore']\n",
    "    other_columns = [col for col in merged_df.columns if col not in columns_order]\n",
    "    merged_df = merged_df[columns_order + other_columns]\n",
    "    \n",
    "    # Print the merged dataframe\n",
    "    print(merged_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate and print average time_total_s\n",
    "    avg_time_total = merged_df['time_total_s'].mean()\n",
    "    print(f\"\\nAverage time_total_s: {avg_time_total:.2f} seconds\")\n",
    "    # Calculate and print average time_total_s\n",
    "    avg_time_total = merged_df['iterations_since_restore'].mean()\n",
    "    print(f\"\\nAverage iterations_since_restore: {avg_time_total:.2f}\")\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.scatter(merged_df.index, merged_df['iterations_since_restore'])\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Iterations Since Restore')\n",
    "    plt.title('Scatter Plot of Iterations Since Restore')\n",
    "    plt.show()\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m df_ctx \u001b[38;5;241m=\u001b[39m df_ctx\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m df_vanilla \u001b[38;5;241m=\u001b[39m df_vanilla\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_vanilla\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Group by all relevant columns to ensure uniqueness\u001b[39;00m\n\u001b[1;32m     41\u001b[0m groupby_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m# of stores\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArchitecture Class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning Rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarehouse_holding_cost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarehouse_lead_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstores_correlation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/core/reshape/concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m     objs,\n\u001b[1;32m    370\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    379\u001b[0m )\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/core/reshape/concat.py:612\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m         obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels\u001b[38;5;241m.\u001b[39mequals(obj_labels):\n\u001b[0;32m--> 612\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m \u001b[43mobj_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m    616\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    617\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    618\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/core/indexes/base.py:3904\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 3904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_unique_msg)\n\u001b[1;32m   3906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/3',\n",
    "    10: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/10',\n",
    "    20: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/20',\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/ctx/50'\n",
    "}\n",
    "vanilla_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/vanilla/3,\n",
    "    10: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/vanilla/10',\n",
    "    20: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/vanilla/20',\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/diff_primitive/vanilla/50'\n",
    "}\n",
    "\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "\n",
    "shared_conditions = {'warehouse_holding_cost': [0.7], 'warehouse_lead_time': [2], 'stores_correlation': [0.5]}\n",
    "# shared_conditions = {'warehouse_holding_cost': [0.7], 'warehouse_lead_time': [6], 'stores_correlation': [0.5]}\n",
    "# shared_conditions = {'warehouse_holding_cost': [1.0], 'warehouse_lead_time': [2], 'stores_correlation': [0.5]}\n",
    "# shared_conditions = {'warehouse_holding_cost': [1.0], 'warehouse_lead_time': [6], 'stores_correlation': [0.5]}\n",
    "\n",
    "condition_for_ctx = shared_conditions.copy()\n",
    "condition_for_ctx['context'] = [0, 1, 64]\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, condition_for_ctx, custom_data_filler)\n",
    "df_ctx.rename(columns={'context': 'context size', 'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_vanilla = results_interpretor.make_table(vanilla_paths, shared_conditions, custom_data_filler)\n",
    "df_vanilla.rename(columns={'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_vanilla.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "df = pd.concat([df_ctx, df_vanilla])\n",
    "min_test_loss = df.groupby(['# of stores', 'warehouse_holding_cost', 'warehouse_lead_time', 'stores_correlation'])['Test Loss'].transform('min')\n",
    "df['Test Gap %'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %', df.pop('Test Gap %'))\n",
    "df.drop(columns=['warehouse_holding_cost', 'warehouse_lead_time', 'stores_correlation'], inplace=True)\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "\n",
    "print(shared_conditions)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  context size  Learning Rate  Train Loss  Dev Loss  Test Loss  # of runs  Test Gap %  best_train_loss\n",
      "           3     Symmatry_Aware           0.0         0.0100    5.678947  5.667238   5.658443          9    3.297811         5.665088\n",
      "           3     Symmatry_Aware           1.0         0.0100    5.494950  5.499710   5.489791          9    0.218976         5.487189\n",
      "           3     Symmatry_Aware          32.0         0.0010    5.475683  5.493100   5.478564          9    0.014024         5.472106\n",
      "           3     Symmatry_Aware          64.0         0.0001    5.476005  5.493236   5.477796          9    0.000000         5.474725\n",
      "           3            Vanilla           NaN         0.0010    5.485427  5.499762   5.493346         39    0.283868         5.472646\n",
      "           5     Symmatry_Aware           0.0         0.0001    5.292834  5.278422   5.309792          9    3.538914         5.292628\n",
      "           5     Symmatry_Aware           1.0         0.0010    5.114685  5.103980   5.136831          9    0.166232         5.113764\n",
      "           5     Symmatry_Aware          32.0         0.0100    5.105794  5.094029   5.129295          9    0.019300         5.103866\n",
      "           5     Symmatry_Aware          64.0         0.0001    5.102159  5.095146   5.128306          9    0.000000         5.101448\n",
      "           5            Vanilla           NaN         0.0001    5.114913  5.109567   5.145210         10    0.329637         5.111620\n",
      "          20     Symmatry_Aware           0.0         0.0010    5.971279  5.905353   7.352042          9   29.382973         5.965318\n",
      "          20     Symmatry_Aware           1.0         0.0010    5.721358  5.680425   5.687429          9    0.088722         5.718916\n",
      "          20     Symmatry_Aware          32.0         0.0010    5.714095  5.673979   5.687816          9    0.095544         5.707474\n",
      "          20     Symmatry_Aware          64.0         0.0100    5.711388  5.672666   5.682387          9    0.000000         5.694829\n",
      "          20            Vanilla           NaN         0.0001    5.722674  5.700140   5.713550          9    0.548415         5.722674\n",
      "          50     Symmatry_Aware           0.0         0.0010    7.588007  7.393392 133.727600         18 2447.691493         7.476560\n",
      "          50     Symmatry_Aware           1.0         0.0001    5.468956  5.428653   5.473787         18    4.283041         5.466714\n",
      "          50     Symmatry_Aware           2.0         0.0001    5.444762  5.409876   5.454721         18    3.919801         5.441719\n",
      "          50     Symmatry_Aware           4.0         0.0010    5.398603  5.371963   5.413336         18    3.131358         5.398603\n",
      "          50     Symmatry_Aware           8.0         0.0010    5.233877  5.213979   5.251703         18    0.052041         5.231795\n",
      "          50     Symmatry_Aware          16.0         0.0010    5.230278  5.216185   5.248971         18    0.000000         5.225365\n",
      "          50     Symmatry_Aware          32.0         0.0010    5.242711  5.220517   5.257046         18    0.153840         5.238373\n",
      "          50     Symmatry_Aware          64.0         0.0100    5.237406  5.217020   5.252123         18    0.060042         5.233271\n",
      "          50            Vanilla           NaN         0.0010    5.266788  5.266957   5.299236         33    0.957598         5.265562\n",
      "         100     Symmatry_Aware           0.0         0.0001    7.390317  7.347567   7.354566         18   31.270953         7.369595\n",
      "         100     Symmatry_Aware           1.0         0.0010    7.370309  7.329111   7.337137         18   30.959866         7.355880\n",
      "         100     Symmatry_Aware           2.0         0.0001    7.145133  7.127300   7.136206         18   27.373463         7.144222\n",
      "         100     Symmatry_Aware           4.0         0.0001    7.121876  7.106964   7.115791         18   27.009081         7.121685\n",
      "         100     Symmatry_Aware           8.0         0.0010    5.895856  5.877767   5.879652         18    4.945339         5.895856\n",
      "         100     Symmatry_Aware          16.0         0.0001    7.135303  7.124479   7.133623         18   27.327363         7.125782\n",
      "         100     Symmatry_Aware          32.0         0.0001    5.645904  5.629479   5.640435         18    0.675581         5.644966\n",
      "         100     Symmatry_Aware          64.0         0.0010    5.596949  5.594267   5.602585         18    0.000000         5.585413\n",
      "         100            Vanilla           NaN         0.0001    5.652277  5.676761   5.686662         27    1.500682         5.634120\n"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/new_perf/ctx/3',\n",
    "    5: '/user/ml4723/Prj/NIC/ray_results/new_perf/ctx/5',\n",
    "    20: '/user/ml4723/Prj/NIC/ray_results/new_perf/ctx/20',\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/new_perf/ctx/50',\n",
    "    100: '/user/ml4723/Prj/NIC/ray_results/new_perf/ctx/100'\n",
    "}\n",
    "vanilla_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/new_perf/vanilla/3',\n",
    "    5: '/user/ml4723/Prj/NIC/ray_results/new_perf/vanilla/5',\n",
    "    20: '/user/ml4723/Prj/NIC/ray_results/new_perf/vanilla/20',\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/new_perf/vanilla/50',\n",
    "    100: '/user/ml4723/Prj/NIC/ray_results/new_perf/vanilla/100'\n",
    "}\n",
    "\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'context': [0, 1, 2, 4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_ctx.rename(columns={'context': 'context size', 'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_vanilla = results_interpretor.make_table(vanilla_paths, {}, custom_data_filler)\n",
    "df_vanilla.rename(columns={'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_vanilla.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "df = pd.concat([df_ctx, df_vanilla])\n",
    "min_test_loss = df.groupby(['# of stores'])['Test Loss'].transform('min')\n",
    "df['Test Gap %'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %', df.pop('Test Gap %'))\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# for 3, 9 from 512 512 512, 0.01, 0.001, 0.0001\n",
    "# 10 from 512, 512, 512, 0.001, 0.0001\n",
    "# 20 from 128, 128, 128, 0.01, 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  context size  Learning Rate  Train Loss  Dev Loss  Test Loss  # of runs  Test Gap %  best_train_loss\n",
      "           3     Symmatry_Aware           0.0          0.001    5.677065  5.668593   5.659001          5    3.401764         5.670380\n",
      "           3     Symmatry_Aware           1.0          0.010    5.484450  5.489308   5.473140          5    0.005710         5.484236\n",
      "           3     Symmatry_Aware          64.0          0.001    5.480534  5.488657   5.472828          5    0.000000         5.477936\n",
      "           3            Vanilla           NaN          0.001    5.481088  5.490378   5.477924         20    0.093108         5.473786\n"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/new_perf/ctx_large/3',\n",
    "}\n",
    "vanilla_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/new_perf/vanilla_large/3',\n",
    "}\n",
    "\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'context': [0, 1, 64]}, custom_data_filler)\n",
    "df_ctx.rename(columns={'context': 'context size', 'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_vanilla = results_interpretor.make_table(vanilla_paths, {}, custom_data_filler)\n",
    "df_vanilla.rename(columns={'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_vanilla.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "df = pd.concat([df_ctx, df_vanilla])\n",
    "min_test_loss = df.groupby(['# of stores'])['Test Loss'].transform('min')\n",
    "df['Test Gap %'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %', df.pop('Test Gap %'))\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# for 3, 9 from 512 512 512, 0.01, 0.001, 0.0001\n",
    "# 10 from 512, 512, 512, 0.001, 0.0001\n",
    "# 20 from 128, 128, 128, 0.01, 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  context size  Learning Rate  Train Loss  Dev Loss  Test Loss  # of runs  Test Gap %  best_train_loss\n",
      "           3     Symmatry_Aware           0.0          0.010    5.924473  5.930560   5.910807          9    1.531933         5.907070\n",
      "           3     Symmatry_Aware           1.0          0.010    5.834254  5.850992   5.821624          9    0.000000         5.823745\n",
      "           3     Symmatry_Aware          64.0          0.010    5.836678  5.855047   5.828812          9    0.123483         5.809814\n",
      "           3            Vanilla           NaN          0.001    5.836746  5.855643   5.823750         25    0.036534         5.796001\n"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/new_perf/ctx_lead/3',\n",
    "}\n",
    "vanilla_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/new_perf/vanilla_lead/3',\n",
    "}\n",
    "\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'context': [0, 1, 64]}, custom_data_filler)\n",
    "df_ctx.rename(columns={'context': 'context size', 'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_vanilla = results_interpretor.make_table(vanilla_paths, {}, custom_data_filler)\n",
    "df_vanilla.rename(columns={'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_vanilla.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "df = pd.concat([df_ctx, df_vanilla])\n",
    "min_test_loss = df.groupby(['# of stores'])['Test Loss'].transform('min')\n",
    "df['Test Gap %'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %', df.pop('Test Gap %'))\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# for 3, 9 from 512 512 512, 0.01, 0.001, 0.0001\n",
    "# 10 from 512, 512, 512, 0.001, 0.0001\n",
    "# 20 from 128, 128, 128, 0.01, 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  context size  Learning Rate  Train Loss  Dev Loss  Test Loss  # of runs  Test Gap %  best_train_loss\n",
      "           3     Symmatry_Aware           0.0          0.010    5.939833  5.923935   5.904784          9    1.543459         5.921181\n",
      "           3     Symmatry_Aware           1.0          0.010    5.845350  5.842640   5.815340          9    0.005311         5.841207\n",
      "           3     Symmatry_Aware          64.0          0.010    5.837446  5.845638   5.815469          9    0.007529         5.836145\n",
      "           3            Vanilla           NaN          0.001    5.847357  5.845670   5.815031         10    0.000000         5.836385\n"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/new_perf/ctx_large_lead/3',\n",
    "}\n",
    "vanilla_paths = {\n",
    "    3: '/user/ml4723/Prj/NIC/ray_results/new_perf/vanilla_large_lead/3',\n",
    "}\n",
    "\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'context': [0, 1, 64]}, custom_data_filler)\n",
    "df_ctx.rename(columns={'context': 'context size', 'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_vanilla = results_interpretor.make_table(vanilla_paths, {}, custom_data_filler)\n",
    "df_vanilla.rename(columns={'learning_rate': 'Learning Rate'}, inplace=True)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_vanilla.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "df = pd.concat([df_ctx, df_vanilla])\n",
    "min_test_loss = df.groupby(['# of stores'])['Test Loss'].transform('min')\n",
    "df['Test Gap %'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %', df.pop('Test Gap %'))\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# for 3, 9 from 512 512 512, 0.01, 0.001, 0.0001\n",
    "# 10 from 512, 512, 512, 0.001, 0.0001\n",
    "# 20 from 128, 128, 128, 0.01, 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " time_this_iter_s  time_total_s  iterations_since_restore  dev_loss  train_loss  test_loss  timestamp checkpoint_dir_name  done  training_iteration    trial_id                date    pid      hostname        node_ip  time_since_restore  config/n_stores  config/learning_rate  config/context config/overriding_networks config/overriding_outputs  config/samples      logdir\n",
      "       866.872476  54402.285268                        74  5.096229    5.098696   5.128291 1724960215                None False                  74 591b0_00006 2024-08-29_15-36-55 100169 researchgpu04 128.59.240.194        54402.285268                5                0.0010              32                  [context]                 [context]               1 591b0_00006\n",
      "       422.219686 108697.749186                       400  5.110029    5.121015   5.145279 1725019678                None False                 400 591b0_00009 2024-08-30_08-07-58 124243 researchgpu04 128.59.240.194       108697.749186                5                0.0001               1                  [context]                 [context]               1 591b0_00009\n",
      "        32.283575  10160.006249                        48  5.289762    5.300347   5.313292 1724915972                None False                  48 591b0_00004 2024-08-29_03-19-32 100167 researchgpu04 128.59.240.194        10160.006249                5                0.0010               0                  [context]                 [context]               1 591b0_00004\n",
      "        39.054165   4728.035863                       121  5.108098    5.117335   5.142509 1724996013                None False                 121 591b0_00029 2024-08-30_01-33-33  87417 researchgpu04 128.59.240.194         4728.035863                5                0.0010               1                  [context]                 [context]               3 591b0_00029\n",
      "        35.363716   2139.618945                        64  7.015638    7.040701   7.038591 1724918027                None False                  64 591b0_00016 2024-08-29_03-53-47  15447 researchgpu04 128.59.240.194         2139.618945                5                0.0010               0                  [context]                 [context]               2 591b0_00016\n",
      "       726.944024  18401.903243                        49  5.097190    5.102525   5.130372 1724935522                None False                  49 591b0_00019 2024-08-29_08-45-22  23324 researchgpu04 128.59.240.194        18401.903243                5                0.0010              64                  [context]                 [context]               2 591b0_00019\n",
      "       570.967139 129149.027666                       197  5.098028    5.104124   5.131280 1725064679                None False                 197 591b0_00022 2024-08-30_20-37-59  89496 researchgpu04 128.59.240.194       129149.027666                5                0.0001              32                  [context]                 [context]               2 591b0_00022\n",
      "        34.234542   3993.783539                       118  5.283121    5.296304   5.311025 1724913244                None False                 118 591b0_00008 2024-08-29_02-34-04 118117 researchgpu04 128.59.240.194         3993.783539                5                0.0001               0                  [context]                 [context]               1 591b0_00008\n",
      "        38.242898   5159.768604                        18  5.112249    5.104290   5.143947 1724910972                None False                  18 591b0_00003 2024-08-29_01-56-12 100165 researchgpu04 128.59.240.194         5159.768604                5                0.0100              64                  [context]                 [context]               1 591b0_00003\n",
      "       295.537428 105558.706113                       167  5.097773    5.100853   5.131826 1725065783                None False                 167 591b0_00023 2024-08-30_20-56-23  20937 researchgpu04 128.59.240.194       105558.706113                5                0.0001              64                  [context]                 [context]               2 591b0_00023\n",
      "       428.864041  63668.789246                       145  5.095578    5.101604   5.128610 1725085566                None False                 145 591b0_00035 2024-08-31_02-26-06  34123 researchgpu04 128.59.240.194        63668.789246                5                0.0001              64                  [context]                 [context]               3 591b0_00035\n",
      "       731.172038  60122.964335                       112  5.104187    5.114541   5.136960 1724965935                None False                 112 591b0_00005 2024-08-29_17-12-15 100168 researchgpu04 128.59.240.194        60122.964335                5                0.0010               1                  [context]                 [context]               1 591b0_00005\n",
      "        37.264974   8854.242025                        52  6.825070    6.838816   6.854024 1724914666                None False                  52 591b0_00000 2024-08-29_02-57-46 100032 researchgpu04 128.59.240.194         8854.242025                5                0.0100               0                  [context]                 [context]               1 591b0_00000\n",
      "        40.774881   5535.765453                        55  5.098344    5.097449   5.131791 1724911348                None False                  55 591b0_00007 2024-08-29_02-02-28 100170 researchgpu04 128.59.240.194         5535.765453                5                0.0010              64                  [context]                 [context]               1 591b0_00007\n",
      "       258.073963   7070.175957                        29  5.103538    5.107658   5.132108 1724912882                None False                  29 591b0_00002 2024-08-29_02-28-02 100164 researchgpu04 128.59.240.194         7070.175957                5                0.0100              32                  [context]                 [context]               1 591b0_00002\n",
      "       416.612056  40634.498676                        74  5.098359    5.104152   5.130150 1725034526                None False                  74 591b0_00030 2024-08-30_12-15-26  94252 researchgpu04 128.59.240.194        40634.498676                5                0.0010              32                  [context]                 [context]               3 591b0_00030\n",
      "        43.291758   3430.331196                        41  5.304905    5.314661   5.336771 1724909242                None False                  41 591b0_00001 2024-08-29_01-27-22 100163 researchgpu04 128.59.240.194         3430.331196                5                0.0100               1                  [context]                 [context]               1 591b0_00001\n",
      "        34.346699    919.028163                        28 56.977842   56.970939  57.008910 1724914171                None False                  28 591b0_00012 2024-08-29_02-49-31   3509 researchgpu04 128.59.240.194          919.028163                5                0.0100               0                  [context]                 [context]               2 591b0_00012\n",
      "       263.869200   9256.164037                        22  5.112336    5.105624   5.144117 1724996608                None False                  22 591b0_00027 2024-08-30_01-43-28  78331 researchgpu04 128.59.240.194         9256.164037                5                0.0100              64                  [context]                 [context]               3 591b0_00027\n",
      "        39.134103   1258.296012                        33  5.108239    5.097278   5.141612 1724917112                None False                  33 591b0_00015 2024-08-29_03-38-32  14805 researchgpu04 128.59.240.194         1258.296012                5                0.0100              64                  [context]                 [context]               2 591b0_00015\n",
      "      3005.340013  32490.400728                        51  5.288007    5.293596   5.312116 1725021889                None False                  51 591b0_00028 2024-08-30_08-44-49  83159 researchgpu04 128.59.240.194        32490.400728                5                0.0010               0                  [context]                 [context]               3 591b0_00028\n",
      "       266.310525  56648.963402                       400  5.108012    5.117657   5.141494 1725054421                None False                 400 591b0_00033 2024-08-30_17-47-01 104948 researchgpu04 128.59.240.194        56648.963402                5                0.0001               1                  [context]                 [context]               3 591b0_00033\n",
      "        35.191323  73201.437015                       182  5.287239    5.295929   5.378781 1724991276                None False                 182 591b0_00020 2024-08-30_00-14-36  34105 researchgpu04 128.59.240.194        73201.437015                5                0.0001               0                  [context]                 [context]               2 591b0_00020\n",
      "       577.733186  14458.337504                        26  5.111069    5.105176   5.145524 1724987344                None False                  26 591b0_00026 2024-08-29_23-09-04  50257 researchgpu04 128.59.240.194        14458.337504                5                0.0100              32                  [context]                 [context]               3 591b0_00026\n",
      "       729.623463  50328.987793                       146  5.097249    5.103208   5.128943 1724963219                None False                 146 591b0_00011 2024-08-29_16-26-59   1392 researchgpu04 128.59.240.194        50328.987793                5                0.0001              64                  [context]                 [context]               1 591b0_00011\n",
      "       583.989775   6932.968591                        11 42.572754   42.524224  42.131199 1724972877                None False                  11 591b0_00025 2024-08-29_19-07-57  35905 researchgpu04 128.59.240.194         6932.968591                5                0.0100               1                  [context]                 [context]               3 591b0_00025\n",
      "        39.891622   1702.221728                        44 14.424574   14.601499 574.454687 1724915880                None False                  44 591b0_00013 2024-08-29_03-18-00   7256 researchgpu04 128.59.240.194         1702.221728                5                0.0100               1                  [context]                 [context]               2 591b0_00013\n",
      "       718.165500  13770.237876                        55  5.097179    5.103540   5.131299 1724930465                None False                  55 591b0_00018 2024-08-29_07-21-05  20624 researchgpu04 128.59.240.194        13770.237876                5                0.0010              32                  [context]                 [context]               2 591b0_00018\n",
      "       369.987599  50984.081772                        90  5.288452    5.296062   5.309454 1725047600                None False                  90 591b0_00032 2024-08-30_15-53-20 102072 researchgpu04 128.59.240.194        50984.081772                5                0.0001               0                  [context]                 [context]               3 591b0_00032\n",
      "        38.841288   1172.336120                        31  5.106603    5.101138   5.143697 1724915846                None False                  31 591b0_00014 2024-08-29_03-17-26   9716 researchgpu04 128.59.240.194         1172.336120                5                0.0100              32                  [context]                 [context]               2 591b0_00014\n",
      "        37.043044   5331.495468                       144  5.096771    5.104409   5.130470 1724916687                None False                 144 591b0_00010 2024-08-29_03-31-27 125992 researchgpu04 128.59.240.194         5331.495468                5                0.0001              32                  [context]                 [context]               1 591b0_00010\n",
      "        39.314691   1742.173761                        45  5.099956    5.101364   5.132564 1724997764                None False                  45 591b0_00031 2024-08-30_02-02-44 100043 researchgpu04 128.59.240.194         1742.173761                5                0.0010              64                  [context]                 [context]               3 591b0_00031\n",
      "       377.920326  26163.219347                        48  5.286644    5.294957   5.312670 1724989391                None False                  48 591b0_00024 2024-08-29_23-43-11  28460 researchgpu04 128.59.240.194        26163.219347                5                0.0100               0                  [context]                 [context]               3 591b0_00024\n",
      "       431.341433  77902.840400                       159  5.124829    5.123922   5.160026 1724993883                None False                 159 591b0_00017 2024-08-30_00-58-03  16393 researchgpu04 128.59.240.194        77902.840400                5                0.0010               1                  [context]                 [context]               2 591b0_00017\n",
      "       261.769690 198207.292267                       357  5.112830    5.121539   5.144708 1725128682                None False                 357 591b0_00021 2024-08-31_14-24-42  76661 researchgpu04 128.59.240.194       198207.292267                5                0.0001               1                  [context]                 [context]               2 591b0_00021\n",
      "       260.744453  75190.352053                       222  5.097758    5.101161   5.130050 1725095283                None False                 222 591b0_00034 2024-08-31_05-08-03  30325 researchgpu04 128.59.240.194        75190.352053                5                0.0001              32                  [context]                 [context]               3 591b0_00034\n",
      "\n",
      "Average time_total_s: 36926.90 seconds\n",
      "\n",
      "Average iterations_since_restore: 107.17\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths = [\n",
    "    \"/user/ml4723/Prj/NIC/ray_results/new_perf/ctx/5/run_2024-08-29_00-29-57/experiment_state-2024-08-29_00-29-58.json\",\n",
    "]\n",
    "merged_results = merge_experiment_results(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " store_underage_cost Architecture Class Optimal # of Weeks  Train Loss    Dev Loss  Dev Gap %\n",
      "                   4             Oracle               None -213.665006 -198.463167   0.000000\n",
      "                   4               HDPO               None -161.395985 -150.136966  24.350211\n",
      "                   4    Weekly Forecast             4.7298 -144.881375 -134.072153  32.444819\n",
      "                   4     Fixed Quantile               None -150.145288 -139.585469  29.666814\n",
      "                   4         Newsvendor               None -134.674537 -120.307394  39.380493\n",
      "                   6             Oracle               None -320.510398 -297.704852   0.000000\n",
      "                   6               HDPO               None -255.922236 -236.749878  20.474968\n",
      "                   6    Weekly Forecast             5.0385 -238.705123 -220.377574  25.974477\n",
      "                   6     Fixed Quantile               None -243.081326 -225.572379  24.229526\n",
      "                   6         Newsvendor               None -226.313396 -203.684138  31.581855\n",
      "                   9             Oracle               None -480.778542 -446.567383   0.000000\n",
      "                   9               HDPO               None -401.497958 -370.765647  16.974311\n",
      "                   9    Weekly Forecast             5.3489 -383.658936 -354.185561  20.687096\n",
      "                   9     Fixed Quantile               None -387.604964 -358.896696  19.632129\n",
      "                   9         Newsvendor               None -369.432498 -334.547661  25.084618\n",
      "                  13             Oracle               None -694.469427 -645.050731   0.000000\n",
      "                  13               HDPO               None -599.765248 -553.278809  14.227086\n",
      "                  13    Weekly Forecast             5.5832 -581.378895 -536.701071  16.797076\n",
      "                  13     Fixed Quantile               None -584.685307 -540.876457  16.149780\n",
      "                  13         Newsvendor               None -561.626534 -510.145355  20.913917\n",
      "Results saved to comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "weekly_forecast_paths = {\n",
    "    1: '/user/ml4723/Prj/NIC/ray_results/cons/weekly_forecast_NN',\n",
    "}\n",
    "data_driven_net_paths = {\n",
    "    1: '/user/ml4723/Prj/NIC/ray_results/cons/data_driven_net',\n",
    "}\n",
    "fixed_quantile_paths = {\n",
    "    1: '/user/ml4723/Prj/NIC/ray_results/cons/fixed_quantile',\n",
    "}\n",
    "quantile_nv_paths = {\n",
    "    1: '/user/ml4723/Prj/NIC/ray_results/cons/quantile_nv',\n",
    "}\n",
    "just_in_time_paths = {\n",
    "    1: '/user/ml4723/Prj/NIC/ray_results/cons/just_in_time',\n",
    "}\n",
    "\n",
    "#def custom_data_filler(out_row, reference_row):\n",
    "#    out_row['path'] = reference_row['path']\n",
    "\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_weekly_forecast = results_interpretor.make_table(weekly_forecast_paths, {'store_underage_cost': [4, 6, 9, 13]})\n",
    "df_data_driven_net = results_interpretor.make_table(data_driven_net_paths, {'store_underage_cost': [4, 6, 9, 13]})\n",
    "df_fixed_quantile = results_interpretor.make_table(fixed_quantile_paths, {'store_underage_cost': [4, 6, 9, 13]})\n",
    "df_quantile_nv = results_interpretor.make_table(quantile_nv_paths, {'store_underage_cost': [4, 6, 9, 13]})\n",
    "df_just_in_time = results_interpretor.make_table(just_in_time_paths, {'store_underage_cost': [4, 6, 9, 13]})\n",
    "df_weekly_forecast.insert(1, 'Architecture Class', \"Weekly Forecast\")\n",
    "df_data_driven_net.insert(1, 'Architecture Class', \"HDPO\")\n",
    "df_fixed_quantile.insert(1, 'Architecture Class', \"Fixed Quantile\")\n",
    "df_quantile_nv.insert(1, 'Architecture Class', \"Newsvendor\")\n",
    "df_just_in_time.insert(1, 'Architecture Class', \"Oracle\")\n",
    "\n",
    "optimal_weeks = {\n",
    "    4: 4.7298,\n",
    "    6: 5.0385,\n",
    "    9: 5.3489,\n",
    "    13: 5.5832\n",
    "}\n",
    "df = pd.concat([df_weekly_forecast, df_data_driven_net, df_fixed_quantile, df_quantile_nv, df_just_in_time])\n",
    "\n",
    "# Insert 'Optimal # of Weeks' next to 'Architecture Class' only for Weekly Forecast rows\n",
    "architecture_class_index = df.columns.get_loc('Architecture Class')\n",
    "df.insert(architecture_class_index + 1, 'Optimal # of Weeks', None)\n",
    "df.loc[df['Architecture Class'] == 'Weekly Forecast', 'Optimal # of Weeks'] = df.loc[df['Architecture Class'] == 'Weekly Forecast', 'store_underage_cost'].map(optimal_weeks)\n",
    "\n",
    "min_dev_loss = df.groupby(['store_underage_cost'])['Dev Loss'].transform('min')\n",
    "df['Dev Gap %'] = -((df['Dev Loss'] - min_dev_loss) / min_dev_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Dev Gap %', df.pop('Dev Gap %'))\n",
    "df.drop(columns=['# of stores', '# of runs'], inplace=True)\n",
    "# Define the custom order for Architecture Class\n",
    "architecture_order = ['Oracle', 'HDPO', 'Weekly Forecast', 'Fixed Quantile', 'Newsvendor']\n",
    "# Create a categorical column with the custom order\n",
    "df['Architecture Class'] = pd.Categorical(df['Architecture Class'], categories=architecture_order, ordered=True)\n",
    "# Sort the dataframe\n",
    "df.sort_values(by=['store_underage_cost', 'Architecture Class'], inplace=True)\n",
    "# Move store_underage_cost to the first column\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('store_underage_cost')\n",
    "cols = ['store_underage_cost'] + cols\n",
    "df = df[cols]\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "df.to_csv('results.csv', index=False)\n",
    "print(\"Results saved to comparison_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  context size  samples  learning_rate  Learning Rate  Train Loss   Dev Loss   Test Loss  # of runs   Test Gap %  best_test_loss\n",
      "          50     Symmatry_Aware             8        1         0.0001         0.0001    5.650241   7.421216   10.879798          1    46.751336        8.233992\n",
      "          50     Symmatry_Aware             8        1         0.0010         0.0010   49.578414  44.554602   41.513131          1   639.152508       41.472713\n",
      "          50     Symmatry_Aware             8        1         0.0100         0.0100  288.772125  37.387215   34.642025          1   517.411742       34.642025\n",
      "          50     Symmatry_Aware             8        2         0.0001         0.0001   18.192926  20.490166  119.765963          1   418.095789       29.069559\n",
      "          50     Symmatry_Aware             8        2         0.0010         0.0010   11.473066  11.937699  148.274425          1   223.611793       18.157361\n",
      "          50     Symmatry_Aware             8        2         0.0100         0.0100   28.024498  30.030687   31.800434          1   466.767144       31.800434\n",
      "          50     Symmatry_Aware             8        3         0.0001         0.0001    6.531549   7.544594    7.572536          1    28.894196        7.232055\n",
      "          50     Symmatry_Aware             8        3         0.0010         0.0010    4.935407   5.489219    5.868295          1     4.088656        5.840255\n",
      "          50     Symmatry_Aware             8        3         0.0100         0.0100    5.874333   7.667728    7.272787          1    29.620148        7.272787\n",
      "          50     Symmatry_Aware             8        4         0.0001         0.0001   28.512488  28.563129  580.505500          1   597.711624       39.147528\n",
      "          50     Symmatry_Aware             8        4         0.0010         0.0010   10.667102  11.226515   12.331546          1   119.780494       12.331546\n",
      "          50     Symmatry_Aware             8        4         0.0100         0.0100    9.421230  10.736800   11.634708          1   105.748743       11.544246\n",
      "          50     Symmatry_Aware             8        5         0.0001         0.0001    5.308506   6.731964    6.458480          1    15.107054        6.458480\n",
      "          50     Symmatry_Aware             8        5         0.0010         0.0010  371.209969 374.802188 4028.482000          1 71642.194537     4025.344400\n",
      "          50     Symmatry_Aware             8        5         0.0100         0.0100   13.176546  14.063784   15.321809          1   173.074828       15.321809\n",
      "          50     Symmatry_Aware             8        6         0.0001         0.0001  250.520422 273.357562 3304.407600          1 58755.764619     3302.306600\n",
      "          50     Symmatry_Aware             8        6         0.0010         0.0010  372.061531 370.521469 4021.885200          1 71580.542521     4021.885200\n",
      "          50     Symmatry_Aware             8        6         0.0100         0.0100    9.622794  10.620093   10.846963          1    93.321334       10.846963\n",
      "          50     Symmatry_Aware             8        7         0.0001         0.0001   22.356555  19.708486  750.301350          1   327.451629       23.983655\n",
      "          50     Symmatry_Aware             8        7         0.0010         0.0010   78.877641  20.265104  437.381500          1   286.449432       21.683084\n",
      "          50     Symmatry_Aware             8        7         0.0100         0.0100   11.765277  13.156447   13.925669          1   148.191940       13.925669\n",
      "          50     Symmatry_Aware             8        8         0.0001         0.0001    5.305539   6.903491   15.854022          1    47.404145        8.270620\n",
      "          50     Symmatry_Aware             8        8         0.0010         0.0010  386.431438 385.491375 4026.495600          1 71641.239245     4025.290800\n",
      "          50     Symmatry_Aware             8        8         0.0100         0.0100   24.799461  25.894732   29.066488          1   418.041040       29.066488\n",
      "          50     Symmatry_Aware             8        9         0.0001         0.0001    6.547800   8.209904  141.290850          1    98.819223       11.155441\n",
      "          50     Symmatry_Aware             8        9         0.0010         0.0010   27.213432  28.200297   28.690756          1   411.344524       28.690756\n",
      "          50     Symmatry_Aware             8        9         0.0100         0.0100   26.591895  29.586049 1654.980600          1   528.905273       35.286909\n",
      "          50     Symmatry_Aware             8       10         0.0001         0.0001   41.082008  42.067441   52.866237          1   831.328390       52.255406\n",
      "          50     Symmatry_Aware             8       10         0.0010         0.0010    8.043410   9.471855   11.244498          1   100.406453       11.244498\n",
      "          50     Symmatry_Aware             8       10         0.0100         0.0100   40.807699  37.777422   35.376450          1   530.501121       35.376450\n",
      "          50     Symmatry_Aware             8       11         0.0001         0.0001    4.469004   6.043322    6.440259          1    14.782313        6.440259\n",
      "          50     Symmatry_Aware             8       11         0.0010         0.0010   18.203174  16.909385   14.924892          1   166.000723       14.924892\n",
      "          50     Symmatry_Aware             8       11         0.0100         0.0100  103.929367  44.510859   40.482025          1   621.495858       40.482025\n",
      "          50     Symmatry_Aware             8       12         0.0001         0.0001   16.372041  18.604625   21.580777          1   284.626039       21.580777\n",
      "          50     Symmatry_Aware             8       12         0.0010         0.0010   24.069021  25.916238   27.561334          1   391.215264       27.561334\n",
      "          50     Symmatry_Aware             8       12         0.0100         0.0100   10.701884  12.710479   12.400822          1   121.015170       12.400822\n",
      "          50     Symmatry_Aware            16        1         0.0001         0.0001    5.506408   6.841063    7.404687          3    31.940639        7.402987\n",
      "          50     Symmatry_Aware            16        1         0.0010         0.0010   20.643801  21.603146   25.726109          3   358.506741       25.726109\n",
      "          50     Symmatry_Aware            16        1         0.0100         0.0100    9.864210  10.839540   11.546058          3   105.274786       11.517653\n",
      "          50     Symmatry_Aware            16        2         0.0001         0.0001    3.836420   5.715986    5.610846          3     0.000000        5.610846\n",
      "          50     Symmatry_Aware            16        2         0.0010         0.0010    5.445050   7.458560    7.999474          3    35.267246        7.589638\n",
      "          50     Symmatry_Aware            16        2         0.0100         0.0100    3.993441   5.917849    6.011600          3     5.739421        5.932877\n",
      "          50     Symmatry_Aware            16        3         0.0001         0.0001    6.711697   7.754762    7.901137          3    35.819455        7.620621\n",
      "          50     Symmatry_Aware            16        3         0.0010         0.0010    6.229695   8.652273    7.818525          3    39.346621        7.818525\n",
      "          50     Symmatry_Aware            16        3         0.0100         0.0100    8.161667   9.768411   10.126743          3    80.485119       10.126743\n",
      "          50     Symmatry_Aware            16        4         0.0001         0.0001    4.612739   6.576154    8.669444          3    42.248910        7.981368\n",
      "          50     Symmatry_Aware            16        4         0.0010         0.0010   15.579573  15.099945   14.410417          3   156.831429       14.410417\n",
      "          50     Symmatry_Aware            16        4         0.0100         0.0100    6.326429   7.503618    7.809777          3    39.190701        7.809777\n",
      "          50     Symmatry_Aware            16        5         0.0001         0.0001    5.217108   7.132089 1759.431000          1    79.301976       10.060359\n",
      "          50     Symmatry_Aware            16        5         0.0010         0.0010   29.327562  31.576428   29.860494          1   432.192314       29.860494\n",
      "          50     Symmatry_Aware            16        5         0.0100         0.0100   41.698164  43.870961   43.100006          1   660.710362       42.682291\n",
      "          50     Symmatry_Aware            16        6         0.0001         0.0001    5.708563   6.964002  177.864763          1    62.269730        9.104705\n",
      "          50     Symmatry_Aware            16        6         0.0010         0.0010    8.373729   9.142083   19.416789          1    78.213473        9.999284\n",
      "          50     Symmatry_Aware            16        6         0.0100         0.0100    7.950964   8.315121   36.431219          1    43.077133        8.027838\n",
      "          50     Symmatry_Aware            16        7         0.0001         0.0001   19.359529  15.134389   13.424016          1   139.251166       13.424016\n",
      "          50     Symmatry_Aware            16        7         0.0010         0.0010  137.592156 159.321719 2059.019800          1 32455.583281     1826.643800\n",
      "          50     Symmatry_Aware            16        7         0.0100         0.0100  139.419125 147.155906 3768.466400          1 47784.831771     2686.744400\n",
      "          50     Symmatry_Aware            16        8         0.0001         0.0001    6.437540   7.522495    8.473835          1    51.025967        8.473835\n",
      "          50     Symmatry_Aware            16        8         0.0010         0.0010   15.194479  16.327463   90.811138          1   242.577041       19.221472\n",
      "          50     Symmatry_Aware            16        8         0.0100         0.0100   33.110082  34.152348   37.939406          1   575.386060       37.894875\n",
      "          50     Symmatry_Aware            16        9         0.0001         0.0001    6.996679   8.088279   61.941456          1   150.528653       14.056778\n",
      "          50     Symmatry_Aware            16        9         0.0010         0.0010    6.096046   8.622121 1803.305000          1   211.651059       17.486262\n",
      "          50     Symmatry_Aware            16        9         0.0100         0.0100    8.399951   9.404387   12.208578          1    88.480585       10.575356\n",
      "          50     Symmatry_Aware            16       10         0.0001         0.0001   12.135149  13.780778   13.359312          1   138.097986       13.359312\n",
      "          50     Symmatry_Aware            16       10         0.0010         0.0010   33.237855  33.796172   31.000369          1   452.507876       31.000369\n",
      "          50     Symmatry_Aware            16       10         0.0100         0.0100   38.650402  36.278094   34.187619          1   509.313030       34.187619\n",
      "          50     Symmatry_Aware            16       11         0.0001         0.0001   40.240441  39.267492   46.693825          1   686.633144       44.136778\n",
      "          50     Symmatry_Aware            16       11         0.0010         0.0010   29.008543  25.089742 1566.727000          1   494.355283       33.348363\n",
      "          50     Symmatry_Aware            16       11         0.0100         0.0100   31.773219  28.435141   24.793225          1   341.880295       24.793225\n",
      "          50     Symmatry_Aware            16       12         0.0001         0.0001   21.404428  23.811055 1298.919500          1   601.887966       39.381856\n",
      "          50     Symmatry_Aware            16       12         0.0010         0.0010    5.986916   6.994790    7.193030          1    28.198668        7.193030\n",
      "          50     Symmatry_Aware            16       12         0.0100         0.0100  468.715625  30.230158   30.563866          1   444.728246       30.563866\n",
      "          50     Symmatry_Aware            32        1         0.0001         0.0001    9.063506   7.329535    7.240467          3    29.044115        7.240467\n",
      "          50     Symmatry_Aware            32        1         0.0010         0.0010    4.279465   6.263754    6.049120          3     3.374010        5.800157\n",
      "          50     Symmatry_Aware            32        1         0.0100         0.0100    5.476728   6.547345    6.998849          3    24.737849        6.998849\n",
      "          50     Symmatry_Aware            32        2         0.0001         0.0001    4.279570   6.034387    6.080084          3     6.682996        5.985819\n",
      "          50     Symmatry_Aware            32        2         0.0010         0.0010    4.000927   5.763334    5.763961          3     1.150527        5.675401\n",
      "          50     Symmatry_Aware            32        2         0.0100         0.0100    7.164728   8.818090    8.680276          3    54.705280        8.680276\n",
      "          50     Symmatry_Aware            32        3         0.0001         0.0001    4.668477   5.925136    6.021928          3     5.107254        5.897407\n",
      "          50     Symmatry_Aware            32        3         0.0010         0.0010    4.733468   6.022953    6.027790          3     5.810941        5.936889\n",
      "          50     Symmatry_Aware            32        3         0.0100         0.0100  505.419312  28.229217   32.091612          3   471.956702       32.091612\n",
      "          50     Symmatry_Aware            32        4         0.0001         0.0001    7.273475   8.395447  109.968412          3    75.808519        9.864346\n",
      "          50     Symmatry_Aware            32        4         0.0010         0.0010   11.487860  11.346823 2711.140000          3   247.199756       19.480845\n",
      "          50     Symmatry_Aware            32        4         0.0100         0.0100    9.125194  10.086215   12.325417          3   103.899687       11.440498\n",
      "          50     Symmatry_Aware            32        5         0.0001         0.0001    5.044583   6.187017 1222.441600          1   124.584122       12.601070\n",
      "          50     Symmatry_Aware            32        5         0.0010         0.0010    7.164386   7.810441    8.152567          1    41.066755        7.915039\n",
      "          50     Symmatry_Aware            32        5         0.0100         0.0100   21.935863  23.840635  114.439662          1   337.020623       24.520556\n",
      "          50     Symmatry_Aware            32        6         0.0001         0.0001   23.700676  26.419158  303.673475          1   487.997898       32.991659\n",
      "          50     Symmatry_Aware            32        6         0.0010         0.0010    6.664550   6.842760    6.887398          1    22.751490        6.887398\n",
      "          50     Symmatry_Aware            32        6         0.0100         0.0100   14.813074  15.817110   15.388809          1   174.268944       15.388809\n",
      "          50     Symmatry_Aware            32        7         0.0001         0.0001   10.670198  11.695595   13.108711          1   133.631609       13.108711\n",
      "          50     Symmatry_Aware            32        7         0.0010         0.0010   10.656840  12.781444   37.818888          1   574.031763       37.818888\n",
      "          50     Symmatry_Aware            32        7         0.0100         0.0100    6.730673  10.835218   11.648098          1   107.599664       11.648098\n",
      "          50     Symmatry_Aware            32        8         0.0001         0.0001   40.898160  43.036219   56.645700          1   900.608935       56.142631\n",
      "          50     Symmatry_Aware            32        8         0.0010         0.0010    3.823449   5.957722    6.311263          1    12.483264        6.311263\n",
      "          50     Symmatry_Aware            32        8         0.0100         0.0100    7.689466   9.223063    9.692743          1    72.750101        9.692743\n",
      "          50     Symmatry_Aware            32        9         0.0001         0.0001   34.803320  35.465512   36.011913          1   540.653277       35.946072\n",
      "          50     Symmatry_Aware            32        9         0.0010         0.0010    4.317927   5.550197    5.808852          1     3.528984        5.808852\n",
      "          50     Symmatry_Aware            32        9         0.0100         0.0100   54.172426  35.004953   35.823178          1   538.462988       35.823178\n",
      "          50     Symmatry_Aware            32       10         0.0001         0.0001    7.177015   8.078966    7.940977          1    41.529043        7.940977\n",
      "          50     Symmatry_Aware            32       10         0.0010         0.0010    5.118780   6.613472 1353.207700          1   388.354343       27.400813\n",
      "          50     Symmatry_Aware            32       10         0.0100         0.0100   10.354162  10.898446   10.862638          1    93.600704       10.862638\n",
      "          50     Symmatry_Aware            32       11         0.0001         0.0001   25.796660  26.602617 4802.689200          1   392.825538       27.651684\n",
      "          50     Symmatry_Aware            32       11         0.0010         0.0010    5.810779   8.294184   50.325972          1   203.620297       17.035669\n",
      "          50     Symmatry_Aware            32       11         0.0100         0.0100    8.978800  10.114109   10.596382          1    88.843108       10.595697\n",
      "          50     Symmatry_Aware            32       12         0.0001         0.0001   15.662164  16.622381   16.467452          1   193.493176       16.467452\n",
      "          50     Symmatry_Aware            32       12         0.0010         0.0010    5.106960   6.851009    7.017880          1    25.077022        7.017880\n",
      "          50     Symmatry_Aware            32       12         0.0100         0.0100  526.984125  37.480102   37.839375          1   574.396904       37.839375\n",
      "          50     Symmatry_Aware            64        1         0.0001         0.0001    4.217728   7.275429    7.200661          3    28.334663        7.200661\n",
      "          50     Symmatry_Aware            64        1         0.0010         0.0010    4.484546   6.741877    6.900670          3     7.498284        6.031564\n",
      "          50     Symmatry_Aware            64        1         0.0100         0.0100    6.461943   6.871795    7.743416          3    38.007989        7.743416\n",
      "          50     Symmatry_Aware            64        2         0.0001         0.0001    3.610964   5.875585    5.889829          3     4.595180        5.868675\n",
      "          50     Symmatry_Aware            64        2         0.0010         0.0010    3.893436   5.809449    5.760008          3     2.658446        5.760008\n",
      "          50     Symmatry_Aware            64        2         0.0100         0.0100    4.007688   5.867419    5.799491          3     2.370339        5.743843\n",
      "          50     Symmatry_Aware            64        3         0.0001         0.0001    5.054910   6.323858    6.602110          3    12.907359        6.335059\n",
      "          50     Symmatry_Aware            64        3         0.0010         0.0010   16.833418  17.514172   20.690561          3   268.760061       20.690561\n",
      "          50     Symmatry_Aware            64        3         0.0100         0.0100   29.635596  30.546563   28.096794          3   400.758554       28.096794\n",
      "          50     Symmatry_Aware            64        4         0.0001         0.0001   15.106633  18.117936   18.260916          2   225.457410       18.260916\n",
      "          50     Symmatry_Aware            64        4         0.0010         0.0010    4.637235   6.073195    6.497418          3    15.801022        6.497418\n",
      "          50     Symmatry_Aware            64        4         0.0100         0.0100   12.245708  12.466879   14.009464          3   149.685392       14.009464\n",
      "          50     Symmatry_Aware            64        5         0.0001         0.0001    4.234126   5.881102    6.032468          1     6.337544        5.966436\n",
      "          50     Symmatry_Aware            64        5         0.0010         0.0010   26.862744  31.551689   30.238125          1   438.922693       30.238125\n",
      "          50     Symmatry_Aware            64        5         0.0100         0.0100    8.379186   9.772137   51.854244          1   139.873008       13.458906\n",
      "          50     Symmatry_Aware            64        6         0.0001         0.0001    6.319443   7.885086  281.843450          1   335.268770       24.422262\n",
      "          50     Symmatry_Aware            64        6         0.0010         0.0010    7.392376   8.286753   11.088516          1    65.160017        9.266875\n",
      "          50     Symmatry_Aware            64        6         0.0100         0.0100   44.056922  37.436027   33.660878          1   499.925131       33.660878\n",
      "          50     Symmatry_Aware            64        7         0.0001         0.0001    5.116526   8.247601    8.290060          1    45.866499        8.184345\n",
      "          50     Symmatry_Aware            64        7         0.0010         0.0010    5.496936   8.745014    9.523438          1    69.732634        9.523438\n",
      "          50     Symmatry_Aware            64        7         0.0100         0.0100   13.769359  13.662096   14.394141          1   156.541338       14.394141\n",
      "          50     Symmatry_Aware            64        8         0.0001         0.0001    6.156161   7.222328    8.058092          1    43.616337        8.058092\n",
      "          50     Symmatry_Aware            64        8         0.0010         0.0010    3.575308   5.249387    5.787091          1     1.805306        5.712139\n",
      "          50     Symmatry_Aware            64        8         0.0100         0.0100    3.829143   5.668832    6.042553          1     6.974744        6.002189\n",
      "          50     Symmatry_Aware            64        9         0.0001         0.0001    3.607748   5.832585    6.730392          1     7.548841        6.034400\n",
      "          50     Symmatry_Aware            64        9         0.0010         0.0010   12.373324  14.167414   14.231997          1   153.651511       14.231997\n",
      "          50     Symmatry_Aware            64        9         0.0100         0.0100   11.463626  11.945296   12.150838          1   116.559806       12.150838\n",
      "          50     Symmatry_Aware            64       10         0.0001         0.0001    7.719349   8.500051    8.490885          1    42.531607        7.997230\n",
      "          50     Symmatry_Aware            64       10         0.0010         0.0010    6.098403   6.444167    6.553852          1    16.347940        6.528104\n",
      "          50     Symmatry_Aware            64       10         0.0100         0.0100   19.885184  21.049133   65.582513          1   385.543827       27.243119\n",
      "          50     Symmatry_Aware            64       11         0.0001         0.0001    4.495500   6.441284    7.396086          1    30.746983        7.336012\n",
      "          50     Symmatry_Aware            64       11         0.0010         0.0010   21.439516  20.527650   36.666088          1   378.925006       26.871747\n",
      "          50     Symmatry_Aware            64       11         0.0100         0.0100    6.134318   7.531006    8.023938          1    39.264484        7.813916\n",
      "          50     Symmatry_Aware            64       12         0.0001         0.0001    4.001245   5.513178    5.935512          1     4.403337        5.857911\n",
      "          50     Symmatry_Aware            64       12         0.0010         0.0010    4.041823   5.542726    6.770159          1    11.738174        6.269457\n",
      "          50     Symmatry_Aware            64       12         0.0100         0.0100    5.156941   5.471878    5.746205          1     2.412438        5.746205\n"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/ctx/50',\n",
    "}\n",
    "gnn_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN/50',\n",
    "}\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    out_row['best_test_loss'] = reference_row['best_test_loss']\n",
    "    pass\n",
    "    # out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "    #out_row['path'] = reference_row['path']\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'context': [1, 2, 4, 8, 16, 32, 64], 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'learning_rate': [0.1, 0.01, 0.001, 0.0001]}, custom_data_filler, sort_by='best_test_loss')\n",
    "df_ctx.rename(columns={'context': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn = results_interpretor.make_table(gnn_paths, {'for_all_networks': [1, 2, 4, 8, 16, 32, 64], 'samples': [1, 2, 3, 4, 5], 'learning_rate': [0.1, 0.01, 0.001, 0.0001]}, custom_data_filler, sort_by='best_test_loss')\n",
    "df_gnn.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_gnn.insert(1, 'Architecture Class', \"GNN\")\n",
    "# df = pd.concat([df_ctx, df_gnn, df_gnn_mp, df_custom_stemb, df_vanilla])\n",
    "df = pd.concat([df_ctx])\n",
    "\n",
    "# Move 'master size' column next to 'context size'\n",
    "if 'master' in df.columns:\n",
    "    context_size_index = df.columns.get_loc('context size')\n",
    "    master_size = df.pop('master')\n",
    "    df.insert(context_size_index + 1, 'master', master_size)\n",
    "\n",
    "min_test_loss = df.groupby(['# of stores'])['best_test_loss'].transform('min')\n",
    "df['Test Gap %'] = ((df['best_test_loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %', df.pop('Test Gap %'))\n",
    "# df.sort_values(by=['# of stores', 'context size', 'master'], inplace=True)\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  context size  Learning Rate  Train Loss  Dev Loss  Test Loss  Test Gap %(all)  Test Gap %(context)  # of runs\n",
      "          50 GNN_Separation_PNA             4         0.0001    5.232822  5.212971   5.230135         0.222787             0.000000          9\n",
      "          50            GNN_PNA             4         0.0010    6.694859  6.672745   6.691896        28.233871            27.948818          9\n",
      "          50 GNN_Separation_PNA             8         0.0001    5.234111  5.213783   5.231656         0.251928             0.149143          9\n",
      "          50            GNN_PNA             8         0.0010    5.227092  5.205777   5.223865         0.102632             0.000000          9\n",
      "          50 GNN_Separation_PNA            16         0.0010    5.230073  5.208341   5.226016         0.143854             0.143854          9\n",
      "          50            GNN_PNA            16         0.0010    5.221111  5.200564   5.218509         0.000000             0.000000          9\n",
      "          50 GNN_Separation_PNA            32         0.0001    5.231083  5.210771   5.228412         0.189762             0.032660          9\n",
      "          50            GNN_PNA            32         0.0001    5.229448  5.208968   5.226705         0.157051             0.000000          9\n",
      "          50 GNN_Separation_PNA            64         0.0001    5.227215  5.207125   5.225103         0.126361             0.024834          9\n",
      "          50            GNN_PNA            64         0.0001    5.225919  5.206423   5.223806         0.101502             0.000000          9\n"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/ctx/50',\n",
    "}\n",
    "gnn_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN/50',\n",
    "}\n",
    "gnn_wistemb_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN_WISTEMB/50',\n",
    "}\n",
    "gnn_pna_wistemb_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN_PNA_WISTEMB/50',\n",
    "}\n",
    "gnn_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN_PNA/50',\n",
    "}\n",
    "gnn_mp_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN_message_passing/50',\n",
    "}\n",
    "gnn_noctx_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN_NoCtx/50',\n",
    "}\n",
    "gnn_noctx_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN_NoCtx_PNA/50',\n",
    "}\n",
    "gnn_separation_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN_Separation/50',\n",
    "}\n",
    "gnn_separation_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/GNN_Separation_PNA/50',\n",
    "}\n",
    "vanilla_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench/vanilla/50',\n",
    "}\n",
    "\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    pass\n",
    "    # out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "    #out_row['path'] = reference_row['path']\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'context': [4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_ctx.rename(columns={'context': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn = results_interpretor.make_table(gnn_paths, {'for_all_networks': [4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_gnn.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_wistemb = results_interpretor.make_table(gnn_wistemb_paths, {'for_all_networks': [4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_gnn_wistemb.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_pna_wistemb = results_interpretor.make_table(gnn_pna_wistemb_paths, {'for_all_networks': [4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_gnn_pna_wistemb.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_pna = results_interpretor.make_table(gnn_pna_paths, {'for_all_networks': [4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_gnn_pna.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_separation = results_interpretor.make_table(gnn_separation_paths, {'for_all_networks': [4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_gnn_separation.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_separation_pna = results_interpretor.make_table(gnn_separation_pna_paths, {'for_all_networks': [4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_gnn_separation_pna.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_mp = results_interpretor.make_table(gnn_mp_paths, {'context': [4, 8, 16, 32, 64]}, custom_data_filler)\n",
    "df_gnn_mp.rename(columns={'context': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_noctx = results_interpretor.make_table(gnn_noctx_paths, {}, custom_data_filler)\n",
    "\n",
    "df_gnn_noctx_pna = results_interpretor.make_table(gnn_noctx_pna_paths, {}, custom_data_filler)\n",
    "\n",
    "df_vanilla = results_interpretor.make_table(vanilla_paths, {'master': [128, 512]}, custom_data_filler)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_gnn.insert(1, 'Architecture Class', \"GNN\")\n",
    "df_gnn_wistemb.insert(1, 'Architecture Class', \"GNN_WISTEMB\")\n",
    "df_gnn_mp.insert(1, 'Architecture Class', \"GNN_MP\")\n",
    "df_gnn_pna_wistemb.insert(1, 'Architecture Class', \"GNN_PNA_WISTEMB\")\n",
    "df_gnn_pna.insert(1, 'Architecture Class', \"GNN_PNA\")\n",
    "df_gnn_separation.insert(1, 'Architecture Class', \"GNN_Separation\")\n",
    "df_gnn_separation_pna.insert(1, 'Architecture Class', \"GNN_Separation_PNA\")\n",
    "df_gnn_noctx.insert(1, 'Architecture Class', \"GNN_NoCtx\")\n",
    "df_gnn_noctx_pna.insert(1, 'Architecture Class', \"GNN_NoCtx_PNA\")\n",
    "df_vanilla.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "# df = pd.concat([df_ctx, df_gnn, df_gnn_wistemb, df_gnn_mp, df_gnn_pna_wistemb, df_gnn_pna, \n",
    "#.                df_gnn_separation, df_gnn_separation_pna, df_gnn_noctx, df_gnn_noctx_pna, df_vanilla])\n",
    "# df = pd.concat([df_ctx])\n",
    "df = pd.concat([df_gnn_separation_pna, df_gnn_pna])\n",
    "\n",
    "# Move 'master size' column next to 'context size'\n",
    "if 'master' in df.columns:\n",
    "    context_size_index = df.columns.get_loc('context size')\n",
    "    master_size = df.pop('master')\n",
    "    df.insert(context_size_index + 1, 'master', master_size)\n",
    "\n",
    "#min_test_loss = df.groupby(['# of stores', 'context size'])['Test Loss'].transform('min')\n",
    "min_test_loss = df.groupby(['# of stores'])['Test Loss'].transform('min')\n",
    "df['Test Gap %(all)'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %(all)', df.pop('Test Gap %(all)'))\n",
    "min_test_loss = df.groupby(['# of stores', 'context size'])['Test Loss'].transform('min')\n",
    "df['Test Gap %(context)'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %(context)', df.pop('Test Gap %(context)'))\n",
    "# df.sort_values(by=['# of stores', 'context size', 'master'], inplace=True)\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  training_n_samples  Train Loss(at best test loss)  Test Loss  Test Gap %  # of runs\n",
      "          50                GNN                   1                       5.538947   6.432312   23.259580        360\n",
      "          50      GNN_Attention                   1                       5.077987   6.142445   17.704976        332\n",
      "          50   GNN_Larger_STEMB                   1                       4.947377   6.359499   21.864285        324\n",
      "          50      GNN_NoCtx_PNA                   1                       6.747120   7.348398   40.814133        108\n",
      "          50            GNN_PNA                   1                       5.321012   6.353386   21.747159        360\n",
      "          50     GNN_ReLU_STEMB                   1                       4.958621   6.475248   24.082342        324\n",
      "          50     Symmatry_Aware                   1                       4.379046   5.959442   14.198182        360\n",
      "          50            Vanilla                   1                       8.540316  23.892081  357.833475        216\n"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/ctx/50',\n",
    "}\n",
    "gnn_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN/50',\n",
    "}\n",
    "gnn_larger_stemb_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_larger_stemb/50',\n",
    "}\n",
    "gnn_relu_stemb_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_ReLU_stemb/50',\n",
    "}\n",
    "gnn_attention_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_attention/50',\n",
    "}\n",
    "gnn_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_PNA/50',\n",
    "}\n",
    "gnn_noctx_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_NoCtx_PNA/50',\n",
    "}\n",
    "vanilla_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/vanilla/50',\n",
    "}\n",
    "\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "    pass\n",
    "    # out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "    #out_row['path'] = reference_row['path']\n",
    "sort_by = 'test_loss'\n",
    "training_n_samples = [1]\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_ctx.rename(columns={'context': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn = results_interpretor.make_table(gnn_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_larger_stemb = results_interpretor.make_table(gnn_larger_stemb_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_larger_stemb.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_relu_stemb = results_interpretor.make_table(gnn_relu_stemb_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_relu_stemb.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_attention = results_interpretor.make_table(gnn_attention_paths, {'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_attention['training_n_samples'] = training_n_samples[0]\n",
    "df_gnn_attention.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_pna = results_interpretor.make_table(gnn_pna_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_pna.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_noctx_pna = results_interpretor.make_table(gnn_noctx_pna_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "\n",
    "df_vanilla = results_interpretor.make_table(vanilla_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_gnn.insert(1, 'Architecture Class', \"GNN\")\n",
    "df_gnn_attention.insert(1, 'Architecture Class', \"GNN_Attention\")\n",
    "df_gnn_pna.insert(1, 'Architecture Class', \"GNN_PNA\")\n",
    "df_gnn_noctx_pna.insert(1, 'Architecture Class', \"GNN_NoCtx_PNA\")\n",
    "df_gnn_relu_stemb.insert(1, 'Architecture Class', \"GNN_ReLU_STEMB\")\n",
    "df_gnn_larger_stemb.insert(1, 'Architecture Class', \"GNN_Larger_STEMB\")\n",
    "df_vanilla.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "# df = pd.concat([df_ctx, df_gnn, df_gnn_wistemb, df_gnn_mp, df_gnn_pna_wistemb, df_gnn_pna, \n",
    "#.                df_gnn_separation, df_gnn_separation_pna, df_gnn_noctx, df_gnn_noctx_pna, df_vanilla])\n",
    "# df = pd.concat([df_ctx])\n",
    "df = pd.concat([df_ctx, df_gnn, df_gnn_attention, df_gnn_pna, df_gnn_noctx_pna, df_gnn_relu_stemb, df_gnn_larger_stemb, df_vanilla])\n",
    "df.drop(columns=['Dev Loss', 'Learning Rate', 'samples'], inplace=True)\n",
    "\n",
    "df_avg = df.groupby(['# of stores', 'Architecture Class', 'training_n_samples']).agg({\n",
    "    'Train Loss': 'mean',\n",
    "    'Test Loss': 'mean',\n",
    "    'best_train_loss': 'mean',\n",
    "    '# of runs': 'sum'\n",
    "}).reset_index()\n",
    "df_avg = df_avg[['# of stores', 'Architecture Class', 'training_n_samples', 'Train Loss', 'Test Loss', '# of runs']]\n",
    "df = df_avg\n",
    "\n",
    "min_test_loss = 5.218509\n",
    "df['Test Gap %'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %', df.pop('Test Gap %'))\n",
    "df.sort_values(by=['# of stores'], inplace=True)\n",
    "df.rename(columns={'Train Loss': 'Train Loss(at best test loss)'}, inplace=True)\n",
    "#df.drop(columns=['# of runs'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  training_n_samples  Train Loss  # of runs\n",
      "          50                GNN                   1    4.560037        360\n",
      "          50      GNN_Attention                   1    4.619906        324\n",
      "          50   GNN_Larger_STEMB                   1    4.625154        324\n",
      "          50      GNN_NoCtx_PNA                   1    4.351136        108\n",
      "          50            GNN_PNA                   1    4.463108        360\n",
      "          50     GNN_ReLU_STEMB                   1    4.644932        324\n",
      "          50     Symmatry_Aware                   1    3.664529        360\n",
      "          50            Vanilla                   1    2.990698        216\n"
     ]
    }
   ],
   "source": [
    "ctx_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/ctx/50',\n",
    "}\n",
    "gnn_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN/50',\n",
    "}\n",
    "gnn_larger_stemb_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_larger_stemb/50',\n",
    "}\n",
    "gnn_relu_stemb_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_ReLU_stemb/50',\n",
    "}\n",
    "gnn_attention_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_attention/50',\n",
    "}\n",
    "gnn_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_PNA/50',\n",
    "}\n",
    "gnn_noctx_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_NoCtx_PNA/50',\n",
    "}\n",
    "vanilla_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/vanilla/50',\n",
    "}\n",
    "\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    pass\n",
    "    # out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "    #out_row['path'] = reference_row['path']\n",
    "sort_by = 'train_loss'\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'training_n_samples': [1], 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_ctx.rename(columns={'context': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn = results_interpretor.make_table(gnn_paths, {'training_n_samples': [1], 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_larger_stemb = results_interpretor.make_table(gnn_larger_stemb_paths, {'training_n_samples': [1], 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_larger_stemb.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_relu_stemb = results_interpretor.make_table(gnn_relu_stemb_paths, {'training_n_samples': [1], 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_relu_stemb.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_attention = results_interpretor.make_table(gnn_attention_paths, {'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_attention['training_n_samples'] = 1\n",
    "df_gnn_attention.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_pna = results_interpretor.make_table(gnn_pna_paths, {'training_n_samples': [1], 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_pna.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_noctx_pna = results_interpretor.make_table(gnn_noctx_pna_paths, {'training_n_samples': [1], 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "\n",
    "df_vanilla = results_interpretor.make_table(vanilla_paths, {'training_n_samples': [1], 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_gnn.insert(1, 'Architecture Class', \"GNN\")\n",
    "df_gnn_attention.insert(1, 'Architecture Class', \"GNN_Attention\")\n",
    "df_gnn_pna.insert(1, 'Architecture Class', \"GNN_PNA\")\n",
    "df_gnn_noctx_pna.insert(1, 'Architecture Class', \"GNN_NoCtx_PNA\")\n",
    "df_gnn_relu_stemb.insert(1, 'Architecture Class', \"GNN_ReLU_STEMB\")\n",
    "df_gnn_larger_stemb.insert(1, 'Architecture Class', \"GNN_Larger_STEMB\")\n",
    "df_vanilla.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "# df = pd.concat([df_ctx, df_gnn, df_gnn_wistemb, df_gnn_mp, df_gnn_pna_wistemb, df_gnn_pna, \n",
    "#.                df_gnn_separation, df_gnn_separation_pna, df_gnn_noctx, df_gnn_noctx_pna, df_vanilla])\n",
    "# df = pd.concat([df_ctx])\n",
    "df = pd.concat([df_ctx, df_gnn, df_gnn_pna, df_gnn_noctx_pna, df_gnn_attention, df_gnn_relu_stemb, df_gnn_larger_stemb, df_vanilla])\n",
    "df.drop(columns=['Test Loss', 'Dev Loss', 'Learning Rate', 'samples'], inplace=True)\n",
    "\n",
    "df_avg = df.groupby(['# of stores', 'Architecture Class', 'training_n_samples']).agg({\n",
    "    'Train Loss': 'mean',\n",
    "    '# of runs': 'sum'\n",
    "}).reset_index()\n",
    "df_avg = df_avg[['# of stores', 'Architecture Class', 'training_n_samples', 'Train Loss', '# of runs']]\n",
    "df = df_avg\n",
    "#df.drop(columns=['# of runs'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  training_n_samples  Train Loss(at best test loss)  Test Loss  Test Gap %  # of runs\n",
      "          50                GNN                 8.0                       9.300430  21.846577  318.636368         16\n",
      "          50      GNN_Attention                 8.0                       6.481350   5.958268   14.175684         13\n",
      "          50      GNN_NoCtx_PNA                 8.0                       5.805554   5.547534    6.304958         12\n",
      "          50            GNN_PNA                 8.0                       6.524081   6.079937   16.507162         12\n",
      "          50     Symmatry_Aware                 8.0                       6.036329   5.916184   13.369240         23\n"
     ]
    }
   ],
   "source": [
    "sort_by = 'test_loss'\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    out_row['best_train_loss'] = reference_row['best_train_loss']\n",
    "    pass\n",
    "\n",
    "ctx_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/ctx/50',\n",
    "}\n",
    "gnn_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN/50',\n",
    "}\n",
    "gnn_attention_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_attention/50',\n",
    "}\n",
    "gnn_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_PNA/50',\n",
    "}\n",
    "gnn_noctx_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_NoCtx_PNA/50',\n",
    "}\n",
    "\n",
    "training_n_samples = [8]\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_ctx.rename(columns={'context': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn = results_interpretor.make_table(gnn_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_attention = results_interpretor.make_table(gnn_attention_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_attention.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_pna = results_interpretor.make_table(gnn_pna_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_pna.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_noctx_pna = results_interpretor.make_table(gnn_noctx_pna_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_gnn.insert(1, 'Architecture Class', \"GNN\")\n",
    "df_gnn_attention.insert(1, 'Architecture Class', \"GNN_Attention\")\n",
    "df_gnn_pna.insert(1, 'Architecture Class', \"GNN_PNA\")\n",
    "df_gnn_noctx_pna.insert(1, 'Architecture Class', \"GNN_NoCtx_PNA\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "# df = pd.concat([df_ctx, df_gnn, df_gnn_wistemb, df_gnn_mp, df_gnn_pna_wistemb, df_gnn_pna, \n",
    "#.                df_gnn_separation, df_gnn_separation_pna, df_gnn_noctx, df_gnn_noctx_pna, df_vanilla])\n",
    "# df = pd.concat([df_ctx])\n",
    "df = pd.concat([df_ctx, df_gnn, df_gnn_attention, df_gnn_pna, df_gnn_noctx_pna])\n",
    "df.drop(columns=['Dev Loss', 'Learning Rate', 'samples'], inplace=True)\n",
    "\n",
    "df_avg = df.groupby(['# of stores', 'Architecture Class', 'training_n_samples']).agg({\n",
    "    'Train Loss': 'mean',\n",
    "    'Test Loss': 'mean',\n",
    "    'best_train_loss': 'mean',\n",
    "    '# of runs': 'sum'\n",
    "}).reset_index()\n",
    "df_avg = df_avg[['# of stores', 'Architecture Class', 'training_n_samples', 'Train Loss', 'Test Loss', '# of runs']]\n",
    "df = df_avg\n",
    "\n",
    "min_test_loss = 5.218509\n",
    "df['Test Gap %'] = ((df['Test Loss'] - min_test_loss) / min_test_loss) * 100\n",
    "df.insert(df.columns.get_loc(df.columns[-2]), 'Test Gap %', df.pop('Test Gap %'))\n",
    "df.sort_values(by=['# of stores'], inplace=True)\n",
    "df.rename(columns={'Train Loss': 'Train Loss(at best test loss)'}, inplace=True)\n",
    "#df.drop(columns=['# of runs'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class  training_n_samples  Train Loss  # of runs\n",
      "          50                GNN                 8.0    7.173801         17\n",
      "          50      GNN_Attention                 8.0    6.300957         16\n",
      "          50      GNN_NoCtx_PNA                 8.0    5.475514         12\n",
      "          50            GNN_PNA                 8.0    6.524081         13\n",
      "          50     Symmatry_Aware                 8.0    5.002208         24\n"
     ]
    }
   ],
   "source": [
    "sort_by = 'train_loss'\n",
    "def custom_data_filler(out_row, reference_row):\n",
    "    pass\n",
    "ctx_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/ctx/50',\n",
    "}\n",
    "gnn_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN/50',\n",
    "}\n",
    "gnn_attention_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_attention/50',\n",
    "}\n",
    "gnn_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_PNA/50',\n",
    "}\n",
    "gnn_noctx_pna_paths = {\n",
    "    50: '/user/ml4723/Prj/NIC/ray_results/stable_bench_sample_efficiency/GNN_NoCtx_PNA/50',\n",
    "}\n",
    "\n",
    "training_n_samples = [8]\n",
    "results_interpretor = rri.RayResultsinterpreter()\n",
    "df_ctx = results_interpretor.make_table(ctx_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_ctx.rename(columns={'context': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn = results_interpretor.make_table(gnn_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_attention = results_interpretor.make_table(gnn_attention_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_attention.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_pna = results_interpretor.make_table(gnn_pna_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "df_gnn_pna.rename(columns={'for_all_networks': 'context size'}, inplace=True)\n",
    "\n",
    "df_gnn_noctx_pna = results_interpretor.make_table(gnn_noctx_pna_paths, {'training_n_samples': training_n_samples, 'samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, custom_data_filler, sort_by=sort_by)\n",
    "\n",
    "df_ctx.insert(1, 'Architecture Class', \"Symmatry_Aware\")\n",
    "df_gnn.insert(1, 'Architecture Class', \"GNN\")\n",
    "df_gnn_attention.insert(1, 'Architecture Class', \"GNN_Attention\")\n",
    "df_gnn_pna.insert(1, 'Architecture Class', \"GNN_PNA\")\n",
    "df_gnn_noctx_pna.insert(1, 'Architecture Class', \"GNN_NoCtx_PNA\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "# df = pd.concat([df_ctx, df_gnn, df_gnn_wistemb, df_gnn_mp, df_gnn_pna_wistemb, df_gnn_pna, \n",
    "#.                df_gnn_separation, df_gnn_separation_pna, df_gnn_noctx, df_gnn_noctx_pna, df_vanilla])\n",
    "# df = pd.concat([df_ctx])\n",
    "df = pd.concat([df_ctx, df_gnn, df_gnn_pna, df_gnn_noctx_pna, df_gnn_attention])\n",
    "df.drop(columns=['Test Loss', 'Dev Loss', 'Learning Rate', 'samples'], inplace=True)\n",
    "\n",
    "df_avg = df.groupby(['# of stores', 'Architecture Class', 'training_n_samples']).agg({\n",
    "    'Train Loss': 'mean',\n",
    "    '# of runs': 'sum'\n",
    "}).reset_index()\n",
    "df_avg = df_avg[['# of stores', 'Architecture Class', 'training_n_samples', 'Train Loss', '# of runs']]\n",
    "df = df_avg\n",
    "#df.drop(columns=['# of runs'], inplace=True)\n",
    "\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('neural_inventory_control')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "295131613949c3a10f810057b447f8c1d2f9ae56e2b2c791b6162e59dbe65d0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
