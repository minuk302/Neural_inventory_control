{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def collect_data(top_folder, ctx_size):\n",
    "    \"\"\" Collects the minimum test loss and corresponding parameters across all subfolders in the main folder. \"\"\"\n",
    "    results = []\n",
    "    # Traverse through each subfolder in the main folder\n",
    "    for submain_folder in os.listdir(top_folder):\n",
    "        main_folder = os.path.join(top_folder, submain_folder)\n",
    "        for subfolder in os.listdir(main_folder):\n",
    "            subfolder_path = os.path.join(main_folder, subfolder)\n",
    "            progress_file = os.path.join(subfolder_path, 'progress.csv')\n",
    "            params_file = os.path.join(subfolder_path, 'params.json')\n",
    "            \n",
    "            # Check if both necessary files exist\n",
    "            if os.path.exists(progress_file) and os.path.exists(params_file):\n",
    "                try:\n",
    "                    # Read progress.csv and find the minimum test loss\n",
    "                    data = pd.read_csv(progress_file)\n",
    "                    data.fillna(0, inplace=True)\n",
    "                    # Read params.json\n",
    "                    with open(params_file, 'r') as file:\n",
    "                        params = json.load(file)\n",
    "                        if params['context'] != ctx_size:\n",
    "                            continue\n",
    "                        # Collect required params and the corresponding test loss\n",
    "                        result = {}\n",
    "                        if 'master_neurons' in params:\n",
    "                            result['master_neurons'] = params.get('master_neurons')\n",
    "                        if 'context_store' in params:\n",
    "                            result['store_embedding_neurons'] = params.get('context_store')\n",
    "                        result['learning_rate'] = params.get('learning_rate')\n",
    "\n",
    "                        result['best_dev_loss'] = data['dev_loss'].min()\n",
    "                        result['test_loss(at best_dev)'] = data[data['dev_loss'] == result['best_dev_loss']]['test_loss'].iloc[0]\n",
    "                        result['train_loss(at best_dev)'] = data[data['dev_loss'] == result['best_dev_loss']]['train_loss'].iloc[0]\n",
    "                        result['best_test_loss'] = data['test_loss'].min()\n",
    "                        result['best_train_loss'] = data['train_loss'].min()\n",
    "                        results.append(result)\n",
    "                    # if 'test_loss' in data.columns:\n",
    "                    #     min_loss = data['test_loss'].min()\n",
    "                    #     # Read params.json\n",
    "                    #     with open(params_file, 'r') as file:\n",
    "                    #         params = json.load(file)\n",
    "                    #         context_size = params.get('context_size')\n",
    "                    #         # Collect required params and the corresponding test loss\n",
    "                    #         result = {\n",
    "                    #             'learning_rate': params.get('learning_rate'),\n",
    "                    #             'best_test_loss': min_loss\n",
    "                    #         }\n",
    "                    #         results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing files in {subfolder_path}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_results_table(main_folder, ctx_size):\n",
    "    \"\"\" Creates a table of the minimum test losses for each combination of learning_rate, context_size, and samples. \"\"\"\n",
    "    data = collect_data(main_folder, ctx_size)\n",
    "    if data:\n",
    "        # Create DataFrame from collected data\n",
    "        df = pd.DataFrame(data)\n",
    "        # Group by the parameters and find the row with the minimum test_loss\n",
    "        sorted_result_df = df.sort_values(by='best_dev_loss', ascending=True)\n",
    "        # print(f'ctx_size : {ctx_size}')\n",
    "        # print(sorted_result_df.head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No data collected. Check the contents of your directories.\")\n",
    "    return sorted_result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_the_result_table(paths):\n",
    "    context_sizes = [0, 1, 256]\n",
    "\n",
    "    # Placeholder for results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each path and context size, call the function, and store the top row\n",
    "    for num_stores, path in paths.items():\n",
    "        for context_size in context_sizes:\n",
    "            df = create_results_table(path, context_size)\n",
    "            top_row = df.iloc[0]  # Get the top row\n",
    "            # Extract necessary columns and add custom columns\n",
    "            result_row = {\n",
    "                \"# of stores\": num_stores,\n",
    "                \"context size\": context_size,\n",
    "                \"Learning Rate\": top_row['learning_rate'],\n",
    "                \"Train Loss\": top_row['train_loss(at best_dev)'],\n",
    "                \"Dev Loss\": top_row['best_dev_loss'],\n",
    "                \"Test Loss\": top_row['test_loss(at best_dev)']\n",
    "            }\n",
    "            results.append(result_row)\n",
    "\n",
    "    # Combine all top rows into a single DataFrame\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/perf/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/perf/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/perf/10\",\n",
    "    20: \"/user/ml4723/Prj/NIC/ray_results/perf/20\",\n",
    "    30: \"/user/ml4723/Prj/NIC/ray_results/perf/30\",\n",
    "    50: \"/user/ml4723/Prj/NIC/ray_results/perf/50\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores  context size  Learning Rate  Train Loss  Dev Loss  Test Loss\n",
      "           3             1         0.0100    5.613812  5.608477   5.609850\n",
      "           3           256         0.0100    5.616237  5.611364   5.612627\n",
      "           5             1         0.0100    5.256996  5.235224   5.247245\n",
      "           5           256         0.0001    5.249202  5.237397   5.249729\n",
      "          10             1         0.0100    5.712263  5.755829   5.720371\n",
      "          10           256         0.0100    5.731197  5.761904   5.729029\n",
      "          20             1         0.0010    5.850531  5.839253   5.818287\n",
      "          20           256         0.0100    5.866663  5.854931   5.832909\n",
      "          30             1         0.0010    5.548159  5.556480   5.550539\n",
      "          30           256         0.0100    5.569959  5.574575   5.569556\n",
      "          50             1         0.0001    6.759303  6.723869   6.789614\n",
      "          50           256         0.0010    5.389271  5.366309   5.386096\n"
     ]
    }
   ],
   "source": [
    "print(make_the_result_table(paths).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('neural_inventory_control')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "295131613949c3a10f810057b447f8c1d2f9ae56e2b2c791b6162e59dbe65d0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
