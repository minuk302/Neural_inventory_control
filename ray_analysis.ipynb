{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def collect_data(top_folder, ctx_size):\n",
    "    \"\"\" Collects the minimum test loss and corresponding parameters across all subfolders in the main folder. \"\"\"\n",
    "    results = []\n",
    "    # Traverse through each subfolder in the main folder\n",
    "    for submain_folder in os.listdir(top_folder):\n",
    "        main_folder = os.path.join(top_folder, submain_folder)\n",
    "        for subfolder in os.listdir(main_folder):\n",
    "            subfolder_path = os.path.join(main_folder, subfolder)\n",
    "            progress_file = os.path.join(subfolder_path, 'progress.csv')\n",
    "            params_file = os.path.join(subfolder_path, 'params.json')\n",
    "            \n",
    "            # Check if both necessary files exist\n",
    "            if os.path.exists(progress_file) and os.path.exists(params_file):\n",
    "                try:\n",
    "                    # Read progress.csv and find the minimum test loss\n",
    "                    data = pd.read_csv(progress_file)\n",
    "                    data.fillna(0, inplace=True)\n",
    "                    # Read params.json\n",
    "                    with open(params_file, 'r') as file:\n",
    "                        params = json.load(file)\n",
    "                        if ctx_size != None and params['context'] != ctx_size:\n",
    "                            continue\n",
    "                        # Collect required params and the corresponding test loss\n",
    "                        result = {}\n",
    "                        if 'master_neurons' in params:\n",
    "                            result['master_neurons'] = params.get('master_neurons')\n",
    "                        if 'context_store' in params:\n",
    "                            result['store_embedding_neurons'] = params.get('context_store')\n",
    "                        result['learning_rate'] = params.get('learning_rate')\n",
    "\n",
    "                        result['best_dev_loss'] = data['dev_loss'].min()\n",
    "                        result['test_loss(at best_dev)'] = data[data['dev_loss'] == result['best_dev_loss']]['test_loss'].iloc[0]\n",
    "                        result['train_loss(at best_dev)'] = data[data['dev_loss'] == result['best_dev_loss']]['train_loss'].iloc[0]\n",
    "                        result['best_test_loss'] = data['test_loss'].min()\n",
    "                        result['best_train_loss'] = data['train_loss'].min()\n",
    "                        result['path'] = subfolder_path\n",
    "                        results.append(result)\n",
    "                    # if 'test_loss' in data.columns:\n",
    "                    #     min_loss = data['test_loss'].min()\n",
    "                    #     # Read params.json\n",
    "                    #     with open(params_file, 'r') as file:\n",
    "                    #         params = json.load(file)\n",
    "                    #         context_size = params.get('context_size')\n",
    "                    #         # Collect required params and the corresponding test loss\n",
    "                    #         result = {\n",
    "                    #             'learning_rate': params.get('learning_rate'),\n",
    "                    #             'best_test_loss': min_loss\n",
    "                    #         }\n",
    "                    #         results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing files in {subfolder_path}: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_results_table(main_folder, ctx_size):\n",
    "    \"\"\" Creates a table of the minimum test losses for each combination of learning_rate, context_size, and samples. \"\"\"\n",
    "    data = collect_data(main_folder, ctx_size)\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        sorted_result_df = df.sort_values(by='best_dev_loss', ascending=True)\n",
    "    else:\n",
    "        print(\"No data collected. Check the contents of your directories.\")\n",
    "    return sorted_result_df\n",
    "\n",
    "def make_the_result_table(paths, context_sizes):\n",
    "    # Placeholder for results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each path and context size, call the function, and store the top row\n",
    "    for num_stores, path in paths.items():\n",
    "        for context_size in context_sizes:\n",
    "            df = create_results_table(path, context_size)\n",
    "            top_row = df.iloc[0]  # Get the top row\n",
    "            # Extract necessary columns and add custom columns\n",
    "            result_row = {\n",
    "                \"# of stores\": num_stores,\n",
    "                \"context size\": context_size,\n",
    "                \"Learning Rate\": top_row['learning_rate'],\n",
    "                \"Train Loss\": top_row['train_loss(at best_dev)'],\n",
    "                \"Dev Loss\": top_row['best_dev_loss'],\n",
    "                \"Test Loss\": top_row['test_loss(at best_dev)'],\n",
    "                # \"path\": top_row['path']\n",
    "            }\n",
    "            results.append(result_row)\n",
    "\n",
    "    # Combine all top rows into a single DataFrame\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m paths \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m3\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m/user/ml4723/Prj/NIC/ray_results/perf/3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m5\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m/user/ml4723/Prj/NIC/ray_results/perf/5\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39m50\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m/user/ml4723/Prj/NIC/ray_results/perf/50\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m df \u001b[39m=\u001b[39m make_the_result_table(paths, [\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m256\u001b[39;49m])\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mto_string(index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[11], line 76\u001b[0m, in \u001b[0;36mmake_the_result_table\u001b[0;34m(paths, context_sizes)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m num_stores, path \u001b[39min\u001b[39;00m paths\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m context_size \u001b[39min\u001b[39;00m context_sizes:\n\u001b[0;32m---> 76\u001b[0m         df \u001b[39m=\u001b[39m create_results_table(path, context_size)\n\u001b[1;32m     77\u001b[0m         top_row \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]  \u001b[39m# Get the top row\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         \u001b[39m# Extract necessary columns and add custom columns\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 61\u001b[0m, in \u001b[0;36mcreate_results_table\u001b[0;34m(main_folder, ctx_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_results_table\u001b[39m(main_folder, ctx_size):\n\u001b[1;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Creates a table of the minimum test losses for each combination of learning_rate, context_size, and samples. \"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     data \u001b[39m=\u001b[39m collect_data(main_folder, ctx_size)\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m     63\u001b[0m         df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data)\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mcollect_data\u001b[0;34m(top_folder, ctx_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(progress_file) \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(params_file):\n\u001b[1;32m     18\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[39m# Read progress.csv and find the minimum test loss\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m         data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(progress_file)\n\u001b[1;32m     21\u001b[0m         data\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m         \u001b[39m# Read params.json\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:232\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    230\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m     \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:365\u001b[0m, in \u001b[0;36m_concatenate_chunks\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m    363\u001b[0m arrs \u001b[39m=\u001b[39m [chunk\u001b[39m.\u001b[39mpop(name) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks]\n\u001b[1;32m    364\u001b[0m \u001b[39m# Check each arr for consistent types.\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m dtypes \u001b[39m=\u001b[39m {a\u001b[39m.\u001b[39mdtype \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrs}\n\u001b[1;32m    366\u001b[0m \u001b[39m# TODO: shouldn't we exclude all EA dtypes here?\u001b[39;00m\n\u001b[1;32m    367\u001b[0m numpy_dtypes \u001b[39m=\u001b[39m {x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m dtypes \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_categorical_dtype(x)}\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:365\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    363\u001b[0m arrs \u001b[39m=\u001b[39m [chunk\u001b[39m.\u001b[39mpop(name) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks]\n\u001b[1;32m    364\u001b[0m \u001b[39m# Check each arr for consistent types.\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m dtypes \u001b[39m=\u001b[39m {a\u001b[39m.\u001b[39mdtype \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrs}\n\u001b[1;32m    366\u001b[0m \u001b[39m# TODO: shouldn't we exclude all EA dtypes here?\u001b[39;00m\n\u001b[1;32m    367\u001b[0m numpy_dtypes \u001b[39m=\u001b[39m {x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m dtypes \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_categorical_dtype(x)}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/perf/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/perf/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/perf/10\",\n",
    "    20: \"/user/ml4723/Prj/NIC/ray_results/perf/20\",\n",
    "    30: \"/user/ml4723/Prj/NIC/ray_results/perf/30\",\n",
    "    50: \"/user/ml4723/Prj/NIC/ray_results/perf/50\"\n",
    "}\n",
    "\n",
    "df = make_the_result_table(paths, [0, 1, 256])\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores Architecture Class context size  Learning Rate  Train Loss   Dev Loss  Test Loss\n",
      "           3     Symmetry_Aware            0         0.0100   15.731358  15.748496  38.862098\n",
      "           3     Symmetry_Aware            1         0.0010    6.618040   6.600957   6.594276\n",
      "           3     Symmetry_Aware          256         0.0100    6.215819   6.195688   6.190809\n",
      "           3            Vanilla         None         0.0001    6.204100   6.199395   6.194573\n",
      "           3        Lower bound         None            NaN         NaN        NaN   6.190000\n",
      "           5     Symmetry_Aware            0         0.0010   14.244384  14.159398  34.947054\n",
      "           5     Symmetry_Aware            1         0.0100    6.061970   6.045415   6.036085\n",
      "           5     Symmetry_Aware          256         0.0100    5.764039   5.763302   5.755474\n",
      "           5            Vanilla         None         0.0001    5.755991   5.759228   5.751869\n",
      "           5        Lower bound         None            NaN         NaN        NaN   5.750000\n",
      "          10     Symmetry_Aware            0         0.0010   14.683252  14.683755  38.511749\n",
      "          10     Symmetry_Aware            1         0.0010  194.557231 193.286115 203.884694\n",
      "          10     Symmetry_Aware          256         0.0010    6.073400   6.060689   6.059724\n",
      "          10            Vanilla         None         0.0001    6.113749   6.119316   6.117425\n",
      "          10        Lower bound         None            NaN         NaN        NaN   6.050000\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/transshipment/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/transshipment/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/transshipment/10\",\n",
    "}\n",
    "df_sym = make_the_result_table(paths, [0, 1, 256])\n",
    "df_sym.insert(1, 'Architecture Class', \"Symmetry_Aware\")\n",
    "\n",
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/10\",\n",
    "}\n",
    "df_van = make_the_result_table(paths, [None])\n",
    "df_van.insert(1, 'Architecture Class', \"Vanilla\")\n",
    "\n",
    "lower_bound = [\n",
    "    {\n",
    "                \"# of stores\": 3,\n",
    "                \"Architecture Class\": \"Lower bound\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": None,\n",
    "                \"Dev Loss\": None,\n",
    "                \"Test Loss\": 6.19,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 5,\n",
    "                \"Architecture Class\": \"Lower bound\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": None,\n",
    "                \"Dev Loss\": None,\n",
    "                \"Test Loss\": 5.75,\n",
    "            },\n",
    "    {\n",
    "                \"# of stores\": 10,\n",
    "                \"Architecture Class\": \"Lower bound\",\n",
    "                \"context size\": None,\n",
    "                \"Learning Rate\": None,\n",
    "                \"Train Loss\": None,\n",
    "                \"Dev Loss\": None,\n",
    "                \"Test Loss\": 6.05,\n",
    "            },\n",
    "]\n",
    "df_lower_bound = pd.DataFrame(lower_bound)\n",
    "\n",
    "df = pd.concat([df_sym, df_van, df_lower_bound])\n",
    "df.sort_values(by=['# of stores', 'context size'], inplace=True)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of stores context size  Learning Rate  Train Loss  Dev Loss  Test Loss\n",
      "           3         None         0.0001    6.206678  6.200491   6.195581\n",
      "           5         None         0.0001    5.755991  5.759228   5.751869\n",
      "          10         None         0.0001    6.152874  6.159148   6.157146\n"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    3: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/3\",\n",
    "    5: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/5\",\n",
    "    10: \"/user/ml4723/Prj/NIC/ray_results/transshipment/vanilla/10\",\n",
    "}\n",
    "\n",
    "df = make_the_result_table(paths, [None])\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('neural_inventory_control')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "295131613949c3a10f810057b447f8c1d2f9ae56e2b2c791b6162e59dbe65d0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
